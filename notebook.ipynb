{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "MODEL = \"mistral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything, but some of them are pretty neutron-tactful!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "model = Ollama(model=MODEL)\n",
    "embeddings = OllamaEmbeddings(model=MODEL)\n",
    "print(model.invoke(\"Tell me a joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why did the chicken cross the road?\n",
      "To get to the other side!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "chain = model | parser\n",
    "print(chain.invoke(\"Tell me a joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/isca22.pdf', 'page': 0}, page_content='Increasing Ising Machine Capacity with Multi-Chip\\nArchitectures\\nAnshujit Sharma, Richard Afoakwa, Zeljko Ignjatovic and Michael Huang\\n{anshujitsharma,richard.afoakwa,zeljko.ignjatovic,michael.huang}@rochester.edu\\nDepartment of Electrical and Computer Engineering, University of Rochester\\nRochester, New York, USA\\nABSTRACT\\nNature has inspired a lot of problem solving techniques over the\\ndecades. More recently, researchers have increasingly turned to\\nharnessing nature to solve problems directly. Ising machines are a\\ngood example and there are numerous research prototypes as well\\nas many design concepts. They can map a family of NP-complete\\nproblems and derive competitive solutions at speeds much greater\\nthan conventional algorithms and in some cases, at a fraction of\\nthe energy cost of a von Neumann computer.\\nHowever, physical Ising machines are often fixed in its problem\\nsolving capacity. Without any support, a bigger problem cannot be\\nsolved at all. With a simple divide-and-conquer strategy, it turns\\nout, the advantage of using an Ising machine quickly diminishes. It\\nis therefore desirable for Ising machines to have a scalable archi-\\ntecture where multiple instances can collaborate to solve a bigger\\nproblem. We then discuss scalable architecture design issues which\\nlead to a multiprocessor Ising machine architecture. Experimental\\nanalyses show that our proposed architectures allow an Ising ma-\\nchine to scale in capacity and maintain its significant performance\\nadvantage (about 2200x speedup over a state-of-the-art computa-\\ntional substrate). In the case of communication bandwidth-limited\\nsystems, our proposed optimizations in supporting batch mode\\noperation can cut down communication demand by about 4-5x\\nwithout a significant impact on solution quality.\\nCCS CONCEPTS\\n•Computer systems organization →Analog computers .\\nKEYWORDS\\nIsing machine, scaling, multi-chip, nature-based computing\\nACM Reference Format:\\nAnshujit Sharma, Richard Afoakwa, Zeljko Ignjatovic and Michael Huang.\\n2022. Increasing Ising Machine Capacity with Multi-Chip Architectures. In\\nThe 49th Annual International Symposium on Computer Architecture (ISCA\\n’22), June 18–22, 2022, New York, NY, USA. ACM, New York, NY, USA, 14 pages.\\nhttps://doi.org/10.1145/3470496.3527414\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than ACM\\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\\nfee. Request permissions from permissions@acm.org.\\nISCA ’22, June 18–22, 2022, New York, NY, USA\\n©2022 Association for Computing Machinery.\\nACM ISBN 978-1-4503-8610-4/22/06. . . $15.00\\nhttps://doi.org/10.1145/3470496.35274141 INTRODUCTION\\nNature apparently does a lot of computation all the time, solving\\ndifferential equations, performing random sampling, and so on.\\nWe have harnessed some of it of course. The transistors, for exam-\\nple, can be turned on/off and are the foundation for most of our\\ncomputers today. But this is different from harnessing nature’s com-\\nputational capability at some higher level, for example, to solve an\\nentire problem. Indeed, some very powerful algorithms are inspired\\nby nature [ 2,33,59]. It is not hard to imagine that if a computing\\nsubstrate is nature-based, we could solve a certain set of problems\\nmuch more quickly and efficiently than through mapping it to the\\nvon Neumann interface. One particular branch of this effort that\\nhas seen some recent rapid advance is Ising machines.\\nIn a nutshell, Ising machines leverage nature to seek low-energy\\nstates for a system of coupled spins. A number of problems (in'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 0}, page_content='has seen some recent rapid advance is Ising machines.\\nIn a nutshell, Ising machines leverage nature to seek low-energy\\nstates for a system of coupled spins. A number of problems (in\\nfact, all original NP-complete problems [ 36]) can be expressed as\\nan equivalent optimization problem of the Ising formula (more on\\nthat in Sec. 2). Though existing Ising machines are largely in the\\nform of prototypes and concepts, they are already showing promise\\nof (much) better performance and energy efficiency for specific\\nproblems.\\nHowever, when the problem size is beyond the capacity of the\\nmachine, the problem can no longer be mapped to the hardware.\\nIntuitively, with some form of divide and conquer, we can create\\nsmaller sub-problems that can map to the hardware and thus still\\nbenefit from the acceleration of an Ising machine. As it turns out,\\nwith the state-of-the-art algorithm employed by D-Wave [ 13], the\\neffective speedup of a system employing such a divide-and-conquer\\nstrategy quickly diminishes as the size of the problem increases. As\\nan example – and more details in Sec. 3.1 – a 500-node machine\\ncan reach a speedup of 600,000 over a von Neumann solver (simu-\\nlated annealing); using the same machine to help solve a 520-node\\nproblem will only achieve a speedup of 250.\\nIn this paper, we first analyze the problem of simple divide-and-\\nconquer strategy. We show why such a strategy is fundamentally\\nlimited in its performance by “glue” computation. Thus, we need\\nmachines that are designed from ground up to obviate such glue.\\nNext, we investigate hardware designs to achieve that goal. Later\\non, we perform experimental analyses that show the design can\\nindeed scale to larger problems while maintaining high perfor-\\nmance; achieving more than 6 orders of magnitude speedup over a\\nsequential solver and over 2200x speedup over a state-of-the-art\\ncomputational accelerator.\\n1'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 1}, page_content='ISCA ’22, June 18–22, 2022, New York, NY, USA Anshujit Sharma, et al.\\n2 BACKGROUND AND RELATED WORK\\n2.1 Principles of Ising machines\\nThe Ising model is used to describe the Hamiltonian of a system\\nof coupled spins. Each spin has one degree of freedom and takes\\none of two values ( 𝜎𝑖∈{− 1,1}). The energy of the system is a\\nfunction of pair-wise coupling of the spins ( 𝐽𝑖𝑗=𝐽𝑗𝑖) and the inter-\\naction of some external field ( 𝜇) with each spin ( ℎ𝑖). The resulting\\nHamiltonian is as follows:\\n𝐻=−Õ\\n𝑖,𝑗𝐽𝑖𝑗𝜎𝑖𝜎𝑗−𝜇Õ\\n𝑖ℎ𝑖𝜎𝑖 (1)\\nGiven such a formulation, a minimization problem can be stated:\\nwhat state of the system ( [𝜎1,𝜎2,...]) has the lowest energy. A\\nphysical system with such a Hamiltonian naturally tends towards\\nlow-energy states. It is as if nature always tries to solve the mini-\\nmization problem, which is not a trivial task. Indeed, the cardinality\\nof the state space grows exponentially with the number of spins,\\nand the optimization problem is NP-complete: it is easily convert-\\nible to and from a generalized max-cut problem, which is part of\\nthe original list of NP-complete problems [30].\\nThus, if a physical system of spins somehow offers programmable\\ncoupling parameters ( 𝐽𝑖𝑗,𝜇andℎ𝑖in Eq. 1), they can be used as a\\nspecial purpose computer to solve optimization problems that can\\nbe expressed in Ising formula (Eq. 1). In fact, all problems in the\\nKarp NP-complete set have their Ising formula derived [ 36]. Also,\\nif a problem already has a QUBO (quadratic unconstrained binary\\noptimization) formulation, mapping to Ising formula is as easy as\\nsubstituting bits ( 𝑏𝑖∈{0,1}) for spins:𝜎𝑖=2𝑏𝑖−1.\\nBecause of the broad class of problems that can map to the Ising\\nformula, building nature-based computing systems that solve these\\nproblems has attracted significant attention [ 6,9,11,14,26,31,32,\\n41,56]. Loosely speaking, an Ising machine’s design goes through\\nfour steps:\\n(1)Identify the physical variable to represent a spin (be it a\\nqubit [ 27], the phase of an optical pulse [ 28], or the polarity\\nof a capacitor [3]);\\n(2)Identify the mechanism of coupling and how to program the\\ncoefficients;\\n(3)Demonstrate the problem solving capability showing both\\nthe theory of its operation (reveal the “invisible hand\" of\\nnature) and satisfactory results of practice;\\n(4)Demonstrate superior machine metrics (solution time, en-\\nergy consumption, and construction costs).\\nIt is important to note that different approaches may offer different\\nfundamental tradeoffs. Each of them may go through varying ges-\\ntation speeds. Thus, it would be premature to evaluate a general\\napproach based on observed instances of prototypes. Nevertheless,\\nwe provide a broad-brushed characterization1, which can help re-\\nsearchers get a basic sense of the landscape – as long as the caveats\\nare properly understood.\\n1This characterization is by no means comprehensive. In particular, for conceptual\\nclarity, we treat the numerous designs that accelerate a von Neumann algorithm\\n(simulated annealing or a variant) using GPU, FPGA, or an ASIC not as a physical\\nIsing machine, but an accelerated simulated annealer [17, 22, 23, 40, 47–49, 57, 58].2.2 The three (and a half) generations of Ising\\nmachines\\nOne of the earliest and perhaps the best known Ising machines are\\nthe quantum annealers marketed by D-Wave. Quantum annealing\\n(QA) is different from adiabatic quantum computing (AQC) in that\\nit relaxes the adiabaticity requirement [ 12]. QA technically includes\\nAQC as a subset, but current D-Wave systems are not adiabatic. In\\nother words, they do not have the theoretical guarantee of reaching\\nground state. Without the ground-state guarantee, the Ising physics\\nof qubits has no other known advantages over alternatives. And\\nit can be argued that using quantum devices to represent spin is\\nperhaps suboptimal. First, the devices are much more sensitive to\\nnoise, necessitating cryogenic operating condition that consumes a\\nlot of power (25KW for D-Wave 2000q). Second, it is perhaps more'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 1}, page_content='perhaps suboptimal. First, the devices are much more sensitive to\\nnoise, necessitating cryogenic operating condition that consumes a\\nlot of power (25KW for D-Wave 2000q). Second, it is perhaps more\\ndifficult to couple qubits than to couple other forms of spins, which\\nexplains why current machines use a local coupling network. The\\nresult is that for general graph topologies, the number of nodes\\nneeded on these locally-coupled machines grow quadratically and\\na nominal 2000 nodes on the D-Wave 2000q is equivalent to only\\nabout 64 effective nodes [24, 25].\\nCoherent Ising machines (CIM) can be thought of as a second-\\ngeneration design where some of the issues are addressed [ 11,28,\\n37,45,55]. In [ 28], all 2000 nodes can be coupled with each other,\\nmaking it apparently the most powerful physical Ising machine\\ntoday. CIM uses special optical pulses serving as spins and therefore\\ncan operate under room temperature and consumes only about\\n200W power. However, besides their own technical challenges, the\\ncurrent CIMs all use computed rather than physical coupling. Such\\na design is essentially a hybrid physical-simulated Ising machine,\\nand is unlikely to be energy-efficient due to fundamental reasons.\\nA recent experiment shows that a 2000 node CIM requires more\\ncomputation than a computational accelerator [48].\\nSince the operating principle of CIM can be viewed with a Ku-\\nramoto model [ 46], using other oscillators can in theory achieve a\\nsimilar goal. This led to a number of electronic oscillator-based Ising\\nmachines (OIM) which can be considered as a third-generation [ 16,\\n51,52]. These systems use LC tanks for spins and (programmable)\\nresistors as coupling units.2These electronic oscillator-based Ising\\nmachines are a major improvement over earlier designs in terms of\\nmachine metrics. To be sure, their exact power consumption and\\noperation speed depend on the exact inductance and capacitance\\nchosen and can thus span a range of orders of magnitude. But it is\\nnot difficult to target a desktop size implementation with around\\n1-10W of power consumption – a significant improvement over\\ncabinet-size machines with a power consumption of 200W-25KW.\\nHowever, for on-chip integration, inductors are often a source of\\npractical challenges. They are area intensive, have undesirable par-\\nasitics with reduced quality factor and increased phase noise all\\nof which pose practical challenges in maintaining frequency uni-\\nformity and phase synchronicity between thousands of on-chip\\noscillators.\\n2Technically, the phase of the oscillators is equivalent to spin with two degrees of\\nfreedom, spanning an XY-plane (rather than the 1 degree of freedom of up or down\\nin the Ising model). Consequently, an additional mechanism is needed to impose a\\nconstraint – such as the Sub-Harmonic Injection Locking (SHIL) [ 15] – to solve Ising\\nformula problems.\\n2'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 2}, page_content='Increasing Ising Machine Capacity with Multi-Chip Architectures ISCA ’22, June 18–22, 2022, New York, NY, USA\\nAnother electronic design with a very different architecture\\n(think of it as generation 3.5) is the Bistable Resistively-coupled\\nIsing Machine (BRIM) [ 3]. In BRIM, the spin is implemented as a\\ncapacitor whose voltage is controlled by a feedback circuit making\\nit bistable. The design is CMOS-compatible and since it uses voltage\\n(as opposed to phase) to represent spin, it enables a straightforward\\ninterface to additional architectural support for computational tasks.\\nWe therefore use a baseline substrate similar to BRIM. Note that\\nthe same principle discussed in the paper could directly apply to\\nall Ising machines with different amount of glue logic.\\n2.3 The common issue\\nFor most state-of-the-art (physical) Ising machines today, when the\\nproblem fits the hardware, significant speedups and energy effi-\\nciency gains can be expected compared to von Neumann computing.\\nHowever, little is discussed on what happens when the problem is\\nbeyond the capacity of the machine. It is understandable to assume\\nthat the machine can still accelerate proportional to the fraction of\\nthe problem that can be mapped. As we will show next, the reality\\nis that for problems beyond the capacity of the machine, we can\\nexpect little to no benefit at all.\\n3 ON THE DIVIDE AND CONQUER\\nSTRATEGY\\nWe first discuss the approach adopted by D-Wave’s system (Sec. 3.1).\\nAs their systems are the only commercially available Ising machine\\nplatforms, their solution is both the state-of-the-art and a baseline\\nfor any comparison. We then discuss the details of a divide-and-\\nconquer strategy, starting with the basic principle (Sec. 3.2) and\\nthen show that the sub-problems to be solved are not independent\\nof each other, making parallelization challenging (Sec. 3.3).\\n3.1 Practice\\nWith D-Wave’s tool [ 13], one can use any Ising machine to solve a\\nproblem larger than its hardware capacity. To see the performance\\nof such a system (for the reader’s convenience, it is replicated in\\nthe appendix as Algorithm 1), we will use a model of BRIM [ 3] as\\nthe Ising machine. Even though the general strategy should work\\nwith any Ising machine, we note that BRIM offers a number of\\npractical advantages for our study. First, it offers all-to-all coupling.\\nThis means that an 𝑛-node machine can map any 𝑛-node arbitrary\\ngraph.3Second, as we explore architectural support for scalable\\nIsing machine later, the CMOS compatibility of BRIM gives us\\nsignificant design flexibility.\\nIn Fig. 1, we show the speedup of an 500-node Ising machine as\\nthe problem size increases past the machine capacity. We omit the\\nmeasurement details as they do not affect the qualitative lessons.\\nFrom the figure, we see two things. First, when the problem gets big-\\nger but still fits within the machine, the speedup increases. Clearly,\\nlarger hardware capacities are desirable.\\nHowever, the figure also shows a second point, and perhaps a\\nmore important point: as soon as the problem is bigger than what\\n3Many machines offer a large number of nominal nodes but only local coupling [ 27,\\n47,57]. A general graph of 𝑛nodes has𝑂(𝑛2)coupling parameters. Mapping such a\\ngraph therefore requires 𝑂(𝑛2)nodes for local connection machines.\\nFigure 1: Speedup of two divide-and-conquer algorithms (D-\\nWave and ours) using a 500-spin BRIM plus a sequential\\ncomputer for support. [Left]: Speedup for all graphs tested.\\n[Right] Magnified segment for graph sizes from 500 to 520.\\nthe hardware can hold, the speedup crashes precipitously (Fig. 1-\\nright). The fact that the speedup reduces is not surprising. What is\\nsurprising is how much and how quickly it does. Of course, some\\nof this is due to the specific implementation of the D-Wave tool.\\nWe have thus created an alternative (Algorithm 2 in the appendix),\\nshown as “Ours” in the figure, and the result (shown in Fig. 1) is\\nonly a small improvement. The sharp drop is due to a fundamental'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 2}, page_content='We have thus created an alternative (Algorithm 2 in the appendix),\\nshown as “Ours” in the figure, and the result (shown in Fig. 1) is\\nonly a small improvement. The sharp drop is due to a fundamental\\nreason, which we delve into next.\\n3.2 Principle\\nWe first start with the principle of divide and conquer in Ising\\noptimization problems. This part may be obvious to some readers\\nwho should skip to Sec. 3.3. The problem of minimizing Eq. 1 is\\noften described as navigating a high-dimensional energy landscape\\nto find the lowest valley. We can imagine keeping some dimensions\\nfixed ( e.g., longitude) and navigate along the remaining dimensions\\nin search of a better spot. Many solvers can be described using this\\nanalogy. This is the essence of the divide and conquer strategy. This\\npoint (as well as its problem) can be shown clearly and explicitly\\nwith some straightforward math. Here, we think the matrix notation\\nis more helpful. We rewrite Eq. 1 as:\\n𝐻=−𝜎⊤𝐽𝜎−𝜇ℎ⊤𝜎 (2)\\nwhere𝜎=[𝜎1,..𝜎𝑛]⊤,𝐽=|𝐽𝑖𝑗|𝑛×𝑛, andℎ=[ℎ1,..,ℎ𝑛]⊤. Here𝐽\\nis a symmetric matrix with the diagonal being 0. If we divide the\\n𝑛-node problem into two sub-problems of 𝑘and𝑛−𝑘nodes, we\\ncan rewrite Eq. 2 as follows.\\n𝐻=−[𝜎⊤\\n𝑢,𝜎⊤\\n𝑙]\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0𝐽𝑢𝐽×\\n𝐽⊤\\n×𝐽𝑙\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\x14𝜎𝑢\\n𝜎𝑙\\x15\\n−𝜇[ℎ⊤\\n𝑢,ℎ⊤\\n𝑙]\\x14𝜎𝑢\\n𝜎𝑙\\x15\\n=−𝜎⊤\\n𝑢𝐽𝑢𝜎𝑢−𝜎⊤\\n𝑙𝐽⊤\\n×𝜎𝑢−𝜎⊤\\n𝑢𝐽×𝜎𝑙−𝜎⊤\\n𝑙𝐽𝑙𝜎𝑙−𝜇ℎ⊤\\n𝑢𝜎𝑢−𝜇ℎ⊤\\n𝑙𝜎𝑙\\n=−\\x10\\n𝜎⊤\\n𝑢𝐽𝑢𝜎𝑢+𝜎⊤\\n𝑙𝐽⊤\\n×𝜎𝑢+𝜇ℎ⊤\\n𝑢𝜎𝑢\\x11\\n−\\x10\\n𝜎⊤\\n𝑙𝐽𝑙𝜎𝑙+𝜎⊤\\n𝑢𝐽×𝜎𝑙+𝜇ℎ⊤\\n𝑙𝜎𝑙\\x11\\n=−\\x10\\n𝜎⊤\\n𝑢𝐽𝑢𝜎𝑢+𝑔⊤\\n𝑢𝜎𝑢\\x11\\n|                   {z                   }\\n𝑠𝑢𝑏−𝑝𝑟𝑜𝑏𝑙𝑒𝑚(𝐽𝑢,𝑔𝑢)−\\x10\\n𝜎⊤\\n𝑙𝐽𝑙𝜎𝑙+𝑔⊤\\n𝑙𝜎𝑙\\x11\\n|                 {z                 }\\n𝑠𝑢𝑏−𝑝𝑟𝑜𝑏𝑙𝑒𝑚(𝐽𝑙,𝑔𝑙)\\n𝑤ℎ𝑒𝑟𝑒𝑔𝑢=𝜇ℎ𝑢+𝐽×𝜎𝑙, 𝑔𝑙=𝜇ℎ𝑙+𝐽⊤\\n×𝜎𝑢(3)\\n3'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 3}, page_content='ISCA ’22, June 18–22, 2022, New York, NY, USA Anshujit Sharma, et al.\\nWith this rewrite, we show that the bigger square matrix can be\\ndecomposed into the upper and lower sub-matrixes 𝐽𝑢and𝐽𝑙(both\\nsquare), and the “cross terms\" ( 𝐽×and its transpose). The effect of\\nthe cross terms can be combined with the original biases ( ℎ𝑢andℎ𝑙\\nrespectively) into new ones ( 𝑔𝑢and𝑔𝑙respectively). From this point\\nof view, an Ising optimization problem with 𝑛nodes can always\\nbe decomposed into sub-problems of 𝑘and𝑛−𝑘nodes, and by\\ntransitivity into a combination of sub-problems of any sizes.\\n3.3 Issues of decomposition\\nEq. 3 not only shows the principle of decomposition, it also clearly\\nshows the issue with it. In the original problem, 𝐽andℎare pa-\\nrameters and do not change. After decomposition, the bias of the\\nupper partition ( 𝑔𝑢) is now a function of the state of the lower par-\\ntition. This means that the two partitions are not independent. In\\nother words, strictly speaking, the sub-problems have to be solved\\nsequentially: when search changes the current state of the upper\\npartition, we need to update the parameters of the lower partition to\\nreflect the change before starting the search in the lower partition.\\nPartitioning also does not reduce total workload. Thus, it is not\\nsurprising that there is no parallel version of canonical simulated\\nannealing through divide-and-conquer.\\nIn the case of trying to solve a bigger problem than a machine’s\\ncapacity, the issue may seem irrelevant: After all, if we can decom-\\npose a bigger problem into two parts (say, A and B), and A now\\nfits into an Ising machine; we can expect to enjoy speedup from\\nthe processing of A even if processing of A and B can not overlap.\\nThe reasoning is correct. But in reality, there are multiple subtle\\nproblems with severe consequences. We will discuss two that are\\nrelevant here:\\n(1)As we already saw, with decomposition, the sub-problem’s\\nformulation changes constantly, which requires reprogram-\\nming. For many Ising machines, reprogramming is a costly\\noperation and can take more time than solving the problem.\\nTo cite a perhaps extreme example, D-Wave’s programming\\ntime is 11.7 ms, compared to a combined 240 𝜇s for the rest\\nof the steps in a typical run [ 19]. Keep in mind, a common\\n(if not universal) usage pattern of these Ising machines is to\\nprogram once and anneal many (e.g., 50) times from different\\ninitial conditions and take the best result. In such a usage\\npattern, long programming time is amortized over many\\nannealing runs. In a decomposed problem, reprogramming\\nmay have to occur many times within one annealing run.\\n(2)Even if the cost of reprogramming is somehow addressed, we\\nstill have the familiar Amdahl’s law. Using a concrete exam-\\nple of BRIM (Fig. 1), the speedup of solving a 500-node prob-\\nlem over a sequential computer is on the orders of 105. Con-\\nsider using our algorithm to decompose a 510-node problem\\ninto a 500-node sub-problem mapped to the hardware and the\\nremaining portion plus glue computation left for software.\\nThe workload for software measured in instruction amounts\\nto about 0.13%of the software workload for the original 510-\\nnode problem. Much of the remaining software workload\\nis the glue (calculating the new biases 𝑔𝑢=𝜇ℎ𝑢+𝐽×𝜎𝑙and\\n𝑔𝑙=𝜇ℎ𝑙+𝐽⊤\\n×𝜎𝑢), different from original solver. As a result,\\nAmdahl’s law does not directly apply. Nonetheless, we can(ab)use it to roughly estimate a speedup upper-bound on the\\norder of 700x.\\nThere are certainly some nuances to the simplified analyses\\nabove, but the bigger point is crystal clear and recapped below.\\n3.4 Recap\\nIn principle, the problem formulation clearly allows decomposition\\nof larger problems. But the smaller component problems are not\\nindependent. As a result, relying on von Neumann computing to\\nglue together multiple Ising machines is a fundamentally flawed\\nstrategy, as it severely limits the acceleration of problems even\\nmarginally larger than a machine’s capacity. The machines need to'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 3}, page_content='glue together multiple Ising machines is a fundamentally flawed\\nstrategy, as it severely limits the acceleration of problems even\\nmarginally larger than a machine’s capacity. The machines need to\\nbe designed from ground up to be used in collaboration and address\\nthe decomposition bottleneck.\\n4 SCALING OF ISING MACHINES\\nOur general approach is conceptually straightforward: make a phys-\\nically bigger Ising machine (spread across multiple chips). We first\\ndiscuss generalities in Sec. 4.1 and a more direct incarnation of a\\nmacro chip and its problems in Sec. 4.2.\\n4.1 General analysis of scaling Ising machines\\n4.1.1 Ising machine basics. The core of an Ising machine contains\\ntwo types of components: nodes and coupling units. As already\\ndiscussed in Sec. 2, the coupling units need to be programmable to\\naccept the coupling strengths 𝐽𝑖𝑗4as the input to the optimization\\nproblem, and the dynamical system will evolve based on some\\nannealing control. Finally, all spins are read out as the solution to\\nthe problem. In a generic problem, any spin can be coupled with any\\nother spin. Thus, there are far more coupling parameters than spins\\n(𝑂(𝑁2)vs𝑂(𝑁)). A number of existing Ising machines, however,\\nadopt a machine architecture where only nearby spins are allowed\\nto couple, resulting in a system with 𝑂(𝑁)coupling units and 𝑂(𝑁)\\nspins [ 27,47,52,56]. A special software tool [ 18] is used to first\\nconvert the original problem into a form that follows the constraint\\nplaced by the machine’s architecture. Loosely speaking, these 𝑂(𝑁)\\ncoupling units can therefore map a problem of the scale of 𝑂(√\\n𝑁)\\nspins. This is confirmed by observation of actual problems [ 3,25].\\nFor the rest of the paper, we will focus only on architectures with\\nall-to-all connections.\\n4.1.2 Electronic Ising machine baseline. A number of electronic\\nIsing machines have been recently proposed [ 16,51,52]. The pri-\\nmary operating principle is similar, though at a deeper level there\\nare significant technical differences. We will discuss the relevant\\noperating principle here. Readers familiar with these machines\\ncan skip forward. Our baseline is BRIM [ 3] where an array of 𝑁\\nbi-stable nodes are interconnected by an array of 𝑁×𝑁resistive\\ncoupling units. The coupling units are programmed by an array\\nof DACs before the system starts annealing. The coupling resistor\\nvalue between nodes 𝑖and𝑗is set to 1/𝐽𝑖𝑗: strong coupling meaning\\nlower resistance. And the sign of coupling strength can be imple-\\nmented with either parallel or antiparallel connection. Specifically,\\nif the coupling parameter 𝐽𝑖𝑗is negative, then the two nodes are\\n4The bias term 𝜇ℎ𝑖𝜎𝑖can be viewed as a special coupling term 𝐽𝑖,𝑛+1𝜎𝑖𝜎𝑛+1(𝐽𝑖,𝑛+1≜\\n𝜇ℎ𝑖𝜎𝑖) which coupled 𝜎𝑖with an extra, fixed spin ( 𝜎𝑛+1=+1).\\n4'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 4}, page_content='Increasing Ising Machine Capacity with Multi-Chip Architectures ISCA ’22, June 18–22, 2022, New York, NY, USA\\nconnected in an antiparallel fashion (positive plate of 𝑖connected to\\nnegative plate of 𝑗and vice versa). This encourages the two nodes\\nto be in opposite polarities, so that the contribution of this pair’s\\ncoupling (−𝐽𝑖𝑗𝜎𝑖𝜎𝑗) lowers the overall energy. Conversely, if 𝐽𝑖𝑗is\\npositive, the plates of the same polarity will be coupled through\\nthe resistor. Fig. 2 shows the machine characteristics.\\nZZZ+−+−+−NotconnectedCoupling(s)𝐽!,#>0−+𝐽!,#<0𝑅!,#∝1𝐽!,#𝑅!,#=𝑅𝐽!,#+−+−+−ConnectedCoupling(s)𝑁𝑜𝑑𝑒\\t𝑖𝑁𝑜𝑑𝑒\\t𝑗\\nFigure 2: A simplified diagram of BRIM showing nodal ca-\\npacitors, coupling resistors, and parallel/antiparallel con-\\nnections.\\nAll these electronic Ising machines can be analyzed as a dynam-\\nical system and Lyapunov stability analysis can show why they\\ntend toward low-energy state in a more theoretical fashion. But a\\nmore intuitive discussion with an example situation suffices for our\\npurpose. Let us imagine the system is in a particular state, and one\\nspin (say,𝜎𝑘=−1) is “wrong” – meaning if we flip it ( 𝜎𝑘=+1),\\nenergy will improve/decrease. A little algebra will show that this\\nmeans\\n𝜎𝑘©\\xad\\n«Õ\\n𝑗≠𝑘𝐽𝑗𝑘𝜎𝑗ª®\\n¬<0 (4)\\nIf we recall 𝐽𝑗𝑘is represented by the coupling resistor between\\nnodes𝑗and𝑘and substitute 𝜎𝑗with the representation of it ( 𝑉𝑗,\\nthe voltage of node 𝑗), the termÍ\\n𝑗≠𝑘𝐽𝑗𝑘𝜎𝑗is thus approximated\\nbyÍ\\n𝑗≠𝑘𝑉𝑗\\n𝑅𝑗𝑘, which describes the current coming into node 𝑘. Ac-\\ncording to Eq. 4, this value is of the opposite sign of 𝜎𝑘. This shows\\nthat if node 𝑘is wrong, the combined current input to it will be in\\nthe opposite polarity and thus has the effect of trying to correct/flip\\nit. A similar exercise can show that when node 𝑘is right (flipping it\\nwould increase/deteriorate energy), the current input from outside\\nnode𝑘will agree with the current polarity of 𝑘, and thus keeping\\nit in the current state. Given this baseline, we now analyze one\\nconceptually straightforward design of an Ising machine with a\\nlarge capacity.\\n4.2 A macrochip architecture\\nFig. 3 shows a k-by-k array of chips each with 𝑁2coupling units\\nthat together form a larger machine with (𝑘𝑁)2coupling units.\\nWe simply connect the wires of a row of coupling units to the\\ncorresponding wires of the same row from the left and/or right\\nneighbor chip. Similarly, the wires of the same column from upper\\nand lower neighbors are joined. If we ignore the packaging and\\nwire loading issue for a moment, we can treat the entire circuit area\\nas one (bigger) macrochip Ising machine with 𝑘𝑁nodes.\\nA single macrochip machine is not necessarily limited to solving\\none problem at a time. Given an Ising machine of 𝑘𝑁nodes, without\\n𝐶𝑈!,#𝐶𝑈!,$𝐶𝑈$,#𝐶𝑈$,$𝐶𝑈#,$𝐶𝑈#,#𝐶𝑈!,#𝐶𝑈!,$𝐶𝑈$,#𝐶𝑈$,$𝐶𝑈#,$𝐶𝑈#,#𝐶𝑈!,#𝐶𝑈!,$𝐶𝑈$,#𝐶𝑈$,$𝐶𝑈#,$𝐶𝑈#,#𝐶𝑈!,#𝐶𝑈!,$𝐶𝑈!,!𝐶𝑈$,#𝐶𝑈$,!𝐶𝑈$,$𝐶𝑈#,!𝐶𝑈#,$𝐶𝑈#,#𝐶𝑈!,!𝐶𝑈$,!𝐶𝑈#,!𝐶𝑈!,!𝐶𝑈$,!𝐶𝑈#,!𝐶𝑈!,!𝐶𝑈$,!𝐶𝑈#,!𝑁!𝑁\"𝑁#𝑁!𝑁\"𝑁#𝑃𝑈\\n𝑃𝑈𝐶ℎ𝑖𝑝!,!𝐶ℎ𝑖𝑝!,\"𝐶ℎ𝑖𝑝\",!𝐶ℎ𝑖𝑝\",\"𝑃𝑈\\n𝑃𝑈Figure 3: Block diagram of a macrochip of k-by-k array (k=2)\\nof chips. Only the leftmost chips need nodes. CU’s are Cou-\\npling Units and PU’s are Programming Units.\\nany system support, a user can already solve multiple smaller prob-\\nlems simultaneously simply by manipulating the coupling matrix –\\nso long as the sum of the number of nodes from each problem does\\nnot exceed𝑘𝑁. This can be seen in the illustration shown in Fig. 4.\\n………………………J1,1…J1,2…………J1,NJ2,1J2,2…J2,NJN,1JN,2…JN,NJN+1,N+1JN+1,N+2JN+1,2N…JN+2,N+1JN+2,N+2JN+2,2N…J2N,N+1J2N,N+2J2N,2N…J2N+1,2N+1J2N+1,2N+2J2N+1,3N…J2N+2,2N+1J2N+2,2N+2J2N+2,3N…J3N,2N+1J3N,2N+2J3N,3N000000\\nFigure 4: Conceptual view showing wasted resource when a\\nsingle macrochip solves multiple smaller problems.\\nIf we keep the shaded area of the coupling matrix all zero,\\nthe matrix is effectively isolated into several smaller submatrixes.\\nThis is obviously not resource-efficient: it takes 𝑘2chip to form a\\nmacrochip of 𝑘𝑁nodes. If we use this macrochip to solve smaller'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 4}, page_content='This is obviously not resource-efficient: it takes 𝑘2chip to form a\\nmacrochip of 𝑘𝑁nodes. If we use this macrochip to solve smaller\\nproblems of size 𝑁, we can only accommodate 𝑘such problems.\\nIndeed, in that specific case, only the coupling arrays of the chips\\nin the diagonal of the 𝑘×𝑘array are being used.\\nSuch waste is not difficult to avoid. One approach is to introduce\\nsome reconfigurability, so that the chips can either work together to\\nsolve a bigger problem or independently to solve multiple smaller\\nproblems. Fig. 5 demonstrates this functionality. Three types of\\nunits need to be reconfigurable in such a design: the nodes, the\\ndiagonal couplers, and the interface pins in each chip. In indepen-\\ndent operation, each chip is isolated from others and operate just\\nlike a single-chip Ising machine: the nodes are in regular mode,\\nthe pins are disconnected from rows and columns of wires, and\\nthe diagonal couplers are in cross-over mode (where the wires for\\nrow𝑖and column 𝑖are connected at the diagonal coupler ( 𝑖,𝑖)). In\\ncollective operation, corresponding rows and columns of wires are\\nconnected through the pins to neighboring chips; all the diagonal\\ncouplers except on the main diagonal of the entire macrochip will\\nbe switched to regular coupler mode (the main diagonal will remain\\n5'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 5}, page_content='ISCA ’22, June 18–22, 2022, New York, NY, USA Anshujit Sharma, et al.\\nSwitch AZ+−Switch BCoupling(s) Disabled\\nNode(s) Regular ModeSwitch BCoupling(s) Enabled\\nSwitch AZ+−Node(s) By-pass Mode\\nFigure 5: Macrochip with switching support to allow either\\nindependent or joint operation. In this specific configura-\\ntion, each chip operates jointly to solve one large problem.\\nin cross-over mode); and only nodes in the leftmost chips are in\\nregular mode while other nodes on other chips will be in by-pass\\nmode. Note that it is also possible that a subset of the chips will be\\nworking collectively while the rest work independently.\\nWhile this macrochip design is conceptually straightforward,\\nthere are a number of issues concerning its implementation. The\\nprimary concern is the chip-interface. Depending on whether the\\nchips are integrated via PCB or interposers, the chip-to-board inter-\\nface may become an engineering challenge. As the interface carries\\nfast-changing analog signals between multiple chips, they certainly\\nmake analysis of system behavior less straightforward. For this\\nreason, the next section will focus on an entirely digital interface.\\nIn a sense, we use multiple chips plus a digital interconnect to make\\na multiprocessor Ising machine.\\n5 A MULTIPROCESSOR ARCHITECTURE\\nBy having all coupling coefficients embodied in physical units, the\\nmacrochip just discussed fundamentally avoids any glue computa-\\ntion to support multi-chip operation. While we keep this essential\\nfeature, the multiprocessor architecture addresses the interface\\nissues of the macrochip.\\n5.1 Basic architecture\\nFig. 6 shows this design. On the top, we have the logical system\\nlayout:𝑁nodes with 𝑁2coupling units. Physically, they can be\\nsplit over, say, 4 chips, each one with a complete set of nodes. Some\\nof these nodes (shown in orange) are merely “shadow copies” of\\nthe real nodes on some other chip. For example, node 3 on chip\\n𝐶1is just a buffer. The real node 3 is on 𝐶2. When node 3 changes\\npolarity (say to -1), 𝐶2will communicate this information to other\\nchips through a digital fabric (not shown). All other chips will use\\n𝐶!𝐶\"𝐶#𝐶$𝑁!𝑁\"𝑁#𝑁$𝑁%𝑁&𝑁\\'𝑁(\\nZZZZZZZZ𝑁!𝑁\"𝑁#Figure 6: Logical view of a single BRIM chip (top) split into\\nmultiple chips (bottom). In the multi-chip setup, nodes are\\nlabelled𝑁1to𝑁8while chips are labelled 𝐶1to𝐶4. Nodes\\nwith orange color are shadow copies, while regular nodes\\nare colored blue. The zoomed in logical views are also shown.\\nThe inter-node digital interconnect is not shown.\\na buffer to maintain a -1 value for node 3 until 𝐶2notifies them\\nof further changes. In other words, compared to the real node, the\\nshadow copies are approximate in time (a bit delayed) and in value\\n(always at a voltage rail).\\n5.2 Reconfigurable chip architecture\\nWith this design, the logical structure of a single chip captures a\\nlong slice of the overall coupling matrix. This logical structure is still\\nimplemented based on a typical square baseline chip architecture.\\nAll we needed to do is to build it from a modular, re-configurable ar-\\nray. As shown in Fig. 7, a single chip can be made into a square array\\nof𝑘×𝑘modules (𝑘=4in the example). Each module consists of an\\narray of𝑛configurable nodes, and 𝑛×𝑛coupling units. The general\\nidea is that these modules can then be strung together differently\\nfor different purposes. For instance, the three configurations are: ①\\n2𝑛×8𝑛;②4𝑛×4𝑛; and③1𝑛×16𝑛. In this way, this chip can be\\nused as a part of a four-chip multiprocessor of 8𝑛nodes total, as a\\nsingle machine of 4𝑛nodes, or part of a 16-chip multiprocessor of\\n16𝑛nodes total.\\nLet us take the configuration of 2𝑛×8𝑛as a concrete example.\\nWhen combined with 3 others chips of the same configuration, the\\nsystem forms a complete 8𝑛×8𝑛coupling matrix. In Fig. 7 (bottom\\nright) we show the desired logical organization of the 16 modules.\\nAmong these modules, only 2 (providing 2𝑛nodes) are configured as\\nregular nodes (module 1 and 2 in blue); 6 are configured as shadow'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 5}, page_content='right) we show the desired logical organization of the 16 modules.\\nAmong these modules, only 2 (providing 2𝑛nodes) are configured as\\nregular nodes (module 1 and 2 in blue); 6 are configured as shadow\\ncopies (3, 4, and 9 to 12 in orange); the rest are configured to pass\\nthrough (green). In addition to different configuration of the nodes,\\nwire connections need to change too. For instance, modules 1 and\\n6'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 6}, page_content='Increasing Ising Machine Capacity with Multi-Chip Architectures ISCA ’22, June 18–22, 2022, New York, NY, USA\\n123456789101112131415164𝑛\\t×\\t4𝑛1𝑛\\t×\\t16𝑛2𝑛\\t×\\t8𝑛Example Chip Configurations234Physical View\\n1 module: 𝑛\\t×\\t𝑛coupling units with 𝑛nodes567891011121314151623411513961014711158121623456789101112131415161\\nRegular NodePassThroughShadowCopyNode Configurations12349101112567813141516Logical View\\nFigure 7: Illustration of a reconfigurable chip made of 4×4\\nmodules each with 𝑛nodes and an array of 𝑛×𝑛coupling\\nunits. The nodes can operate in three different modes: reg-\\nular (blue), shadow copy (orange), or pass-through (green).\\nThese modules can be configured in three ways, correspond-\\ning to different coupling (sub)matrices. ①2𝑛×8𝑛: as can be\\nseen in the logical view (bottom right), the 16 modules are ef-\\nfectively connected as 2 columns each with 8 modules. This\\ncan be used in a 4-chip multiprocessor with a total of 8𝑛\\nnodes. ②4𝑛×4𝑛: the chip is an independent machine with\\n4𝑛nodes and the 16𝑛2coupling units organized as a 4𝑛×4𝑛\\nmatrix. These 4𝑛nodes are in the first column (blue). The\\nnodes in the rest of the modules are set in pass-through\\nmode (green). ③1𝑛×16𝑛: similar to the first example, this\\nchip is used in a 16-chip multiprocessor with a total of 16𝑛\\nnodes.\\n9 have to be connected so that modules 1 to 4 and 9 to 12 can act as\\none 8-module tall column.\\n5.3 Communication demand and technological\\nsolutions\\nThe basic idea is that when a spin changes polarity, one chip needs\\nto communicate to other chips in order for them to update their\\nshadow copies. The communication demand is, to a first approxima-\\ntion,𝑓𝑠𝑁𝑙𝑜𝑔(𝑁), where𝑁is the total number of spins in a system\\nand𝑓𝑠is the frequency of spin flips. Take a concrete example of\\nour baseline Ising substrate: on average, one spin/node flips every\\n10 ns, depending on problems being solved. Assuming the same\\nspin flip frequency, if we take sixteen 8,000-spin chips to form a\\nmultiprocessor Ising machine, the total system would offer 32,000\\nspins (√\\n16×8000 ) and would require at least 50 Tb/s (broadcast)\\nbandwidth. In fact, due to the annealing schedule, the system has\\na higher spin flip frequency at the beginning of the schedule and\\nthus would demand even more peak bandwidth.\\nNote that such communication is also needed for any multi-\\nthreaded von Neumann solver. The difference is that, compared with\\na state-of-the-art physical Ising machine, a von Neumann solver is\\norders-of-magnitude slower and thus has a correspondingly lower\\nbandwidth demand.Given this significant, intrinsic bandwidth demand, a number of\\ntechnological solutions immediately come to mind. Optical commu-\\nnication and 3D integration are both appealing options. Indeed, 3D\\nintegration is a very convenient solution to the proposed architec-\\nture. Fig. 8 shows an example 4-layer 3D IC. We can see nodes and\\ntheir shadow copies are conveniently located on top of each other\\nand thus can be easily connected with through-silicon vias (TSV).\\nIn fact, due to the short distance of the TSVs, shadow nodes are no\\nlonger necessary architecturally. They may still be a convenient\\ncircuit solution for improving driving capabilities though.\\n14151610111256783124Layer 1\\n3D View1191012756815131416Layer 3\\nLayer 2Layer 412345678910111213141516Logical ViewPhysical View1234\\nFigure 8: Illustration of 3D-integrated multiprocessor. Each\\nlayer (physical view) is shown as four modules (each module\\nis an𝑛×𝑛array). When four layers are connected to form a\\nmultiprocessor, every layer operates as an 1𝑛×4𝑛slice (logi-\\ncal view). Together, they form the 4𝑛×4𝑛machine. In the log-\\nical view, within each slice, the blue box indicates the mod-\\nule of regular spins while the orange ones are their shadow\\ncopies ( e.g., block 6’s shadow copies are 2, 10, and 14, which\\nare conveniently on top of each other in the 3D view to be\\nconnected by TSV).\\nFinally, we can always slow down the physics of the Ising ma-\\nchine so that the communication demand matches the supply of'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 6}, page_content='connected by TSV).\\nFinally, we can always slow down the physics of the Ising ma-\\nchine so that the communication demand matches the supply of\\nthe fabric. In the case of BRIM, this can be achieved in a combina-\\ntion of (at least) two ways. First, the machine’s RC constant can be\\nincreased – higher coupling resistors will be used to slow down\\ncharging. Second, the system can be stopped altogether, for in-\\nstance, to wait out a congestion. No matter how we combine these\\nmechanisms the math is simple: to reduce bandwidth demand by 2x,\\nwe need to slow down the machine by 2x. There are things that we\\ncan do to reduce the bandwidth demand without a corresponding\\nreduction in performance. We discuss these next.\\n5.4 Concurrent mode operation\\nConcurrent operation of multiple Ising machines (solving the same\\nproblem) can be roughly described as a combination of each ma-\\nchine performing local searches independently and exchanging\\ninformation about the state of spins with each other. A surpris-\\ningly consequential design parameter is how long to wait before\\ncommunicating a change of spin to others. Sending any change\\nimmediately seems the natural choice as the multiprocessor func-\\ntions most closely to a monolithic, large physical Ising machine.\\nHowever, waiting has its merit too. During a window of time, a\\nspin may flip back and forth multiple times. With some waiting, we\\navoid wasting bandwidth on unnecessary updates. In this regards,\\n7'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 7}, page_content='ISCA ’22, June 18–22, 2022, New York, NY, USA Anshujit Sharma, et al.\\nthe longer the waiting, the more we can save on bandwidth. In\\nreality, however, the wait time has implications on the solution\\nquality, as we explain next.\\n5.4.1 Impact of global state ignorance. In a concurrent operation,\\nevery solver is always having some “ignorance” of the true state\\nof spins mapped on other solvers. Take a 4-solver system as an\\nexample. At any point, the system’s global state can be represented\\nby𝑆𝑔=[𝐴,𝐵,𝐶,𝐷]⊤where each letter represents the spin vector\\nof each machine. Due to communication delays as well as waiting\\nmentioned above, the first solver’s belief of the current state is\\nthus𝑆1=[𝐴,𝐵′,𝐶′,𝐷′]⊤. In its local search, it is essentially trying\\nto optimize the energy of this believed state 𝐸(𝑆1). A low𝐸(𝑆1)\\ndoes not necessarily mean that the energy of the system’s true\\nstate𝐸(𝑆𝑔)is also low. Imagine that solver 1 is now given the true\\nstate of other solvers and recalculates the energy. We can define the\\ndifference as the the energy surprise :𝐸𝑠𝑢𝑟𝑝𝑟𝑖𝑠𝑒 =𝐸(𝑆1)−𝐸(𝑆𝑔).5\\nFig. 9 shows the empirical observations of the energy surprises.\\nFigure 9: Degree of ignorance and the corresponding energy\\nsurprise for different epoch sizes. The graph has 8000 nodes\\nwhich is partitioned and mapped onto 8 solvers with each\\nsolving 1000 nodes. The figure shows the communication for\\nsmall, medium and large epochs . A magnified segment near\\nthe origin is also shown.\\nThis particular experiment is obtained by solving an 8000-node\\nproblem divided into 8 sub-problems each solved by a simulated\\nannealing (SA) solver. After initialization, each solver uses the state\\nof the other 7000 nodes to compute the biases. Then they perform\\nlocal searches for a fixed amount of time (called an epoch) before\\ncommunicating the state with each other. At the epoch boundary,\\nwe can show the amount of ignorance measured by the percentage\\nof external spins that have changed. We also calculate the energy\\nsurprise. Each dot represents the data of one epoch and the figure\\nshows the results of all epochs from 20 runs.\\nWe see that when the epoch time is long, more spin changes occur\\nfrom other solvers. As a result, any single solver is under a higher\\ndegree of ignorance of the external state, leading to a higher degree\\nof misjudgement and a larger magnitude of the energy surprise.\\n5Defined this way, a positive energy surprise means that the current state has lower\\n(better) energy than what the solver believed prior to the update: in other words, it is\\na good/positive surprise.When the epoch is longer than a certain value, the surprise is highly\\ncorrelated with the degree of ignorance. In this regime, one can say\\nthat the parallel solvers are clearly doing a poor job (also reflected in\\nvery poor final solution quality not shown in the graph). So far, this\\nmessage is consistent with earlier analysis that decomposed sub-\\nproblems are not independent from each other. However, when the\\nepoch time goes below a certain threshold (small epoch, also shown\\nin the magnified segment), the situation seems to have gone through\\na phase change: now, the energy surprise is no longer uniformly\\nnegative. At any rate, the magnitude of surprise gets lower. In\\nother words, despite having some ignorance, the solvers can still\\nfind reasonable solutions. In fact, the overall solution quality is\\nno worse (and often statistically better) than running the solvers\\nsequentially ( i.e., without any ignorance). This means that it is\\npractical to operate multiple Ising machines concurrently so long\\nas they can communicate sufficiently swiftly to prevent building\\nup too much ignorance.\\n5.4.2 Coordinated induced spin flips. Another important aspect of\\nthe design is about spin flips introduced to the system to prevent\\nit from being stuck at a local minimum. (We will refer to them as\\ninduced spin flips.) These spin flips are generally applied stochasti-\\ncally, similar to an accepted proposal in the Metropolis algorithm.'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 7}, page_content='induced spin flips.) These spin flips are generally applied stochasti-\\ncally, similar to an accepted proposal in the Metropolis algorithm.\\nIn a practical implementation, randomness is often of a determinis-\\ntic, pseudo-random nature. As a result, if we properly synchronize\\nthe pseudo random number generator (PRNG) on each chip, we\\ncan guarantee that each chip will generate the same output every-\\nwhere at about the same time. In this way, we can apply induced\\nspin flips without explicit communication. In other words, instead\\nof randomly choosing, say, spin node 3 to flip and then sending\\nan explicit message from the chip that contains the node to other\\nchips informing them of the flip, the PRNG on all chips would be\\nprogrammed to induce node 3 (and its shadow copies) to flip and\\nupdate the nodal capacitor or the shadow register at about the same\\ntime.\\nTo sum: phenomenologically multiple solvers canoperate con-\\ncurrently and seems to offer the highest solution quality, provided\\nthey keep each other informed “sufficiently promptly”. This means,\\nwe have to choose a short epoch time, which generally means high\\ncommunication demands. One opportunity to reduce the demand\\nis using properly synchronized PRNG for induced spin flips.\\n5.5 Batch mode operation\\nWhile careful design of concurrent operation can achieve noticeable\\nbandwidth savings (about 1.5x as we shall see), a completely differ-\\nent mode of operation – batch mode – can allow a more substantial\\nsavings (about 5x). This mode leverages the fact that a common, if\\nnot universal, mode of using an annealer is to perform a batch of\\nannealing jobs of the same problem with different initial states and\\ntake the best solution from the batch. Each job is thus independent\\nof each other. Knowing that we have a batch of jobs of the same\\nsetup , we can stagger them in a fairly straightforward manner to\\nreduce the necessary communication.\\nThe key idea is illustrated in Fig. 10. Viewed vertically, a single\\njob (from one initial state, say Job 1) is still spread out over multiple\\nsolvers (on Chip 1 in epoch 1, on Chip 2 in epoch 2, etc.) like in\\nconcurrent mode. So each chip is still just annealing for its part of\\n8'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 8}, page_content='Increasing Ising Machine Capacity with Multi-Chip Architectures ISCA ’22, June 18–22, 2022, New York, NY, USA\\nthe problem. But the solvers now work sequentially. At the end of\\nEpoch 1, Chip 1 passes on the updated spin state to others (indicated\\nin the figure by the change to a darker red for the first quarter\\nof spins in all chips). Chip 2 then picks up Job 1 and continues\\nexploration of the second quarter of the spins.\\nEpoch 1\\nEpoch 2SyncChip 1Chip 2Chip 3Chip 4\\nChip 1Chip 2Chip 3Chip 4Spin copiesDarker shade is updatedJob 1Job 2Job 3Job 4\\nFigure 10: Illustration of batch mode operation where four\\nstaggered job runs of the same problem starting from dif-\\nferent initial conditions are solved by four solvers. Each job\\nrun is depicted with a different color. Each chip has a copy\\nof spins for every job. At the end of the epoch, all the chips\\nbroadcast its spins via dedicated channels which updates all\\nthe spin copies (shown by darker shades). Then each chip\\nloads the next job’s spin states and starts the next epoch of\\nannealing of that job.\\nViewed horizontally, in every epoch each of the 4 chips works\\non a different job (indicated by different colors). In the synchroniza-\\ntion phase, they exchange the updated state and afterward start\\nannealing on a different job. The key advantage of this approach\\nis that each epoch can be much longer in time – without creating\\nany ignorance. As already discussed, with a longer epoch, the to-\\ntal communication bandwidth needed can be much less than that\\nneeded to communicate every single event of spin flip.\\nTo exploit parallelism, in batch mode, 𝑛different jobs (from dif-\\nferent initial states) are performed simultaneously across 𝑛solvers.\\nAs a result, the system as a whole needs to carry 𝑛copies of states\\ninstead of just one in the concurrent mode. To support this, we\\nneed a modest increase in storage ( 𝑛×𝑁bits per solver) to keep\\nthe states for different jobs.\\nFinally, we note that it is tempting to think that a good way to\\nrun batch mode is just like in a von Neumann system where every\\nmachine runs an independent job. This is decidedly less efficient in\\na multiprocessor Ising machine: If an entire problem is solved by\\none machine, we need to context-switch in the new parameters at\\nthe end of every epoch. The data volume is 𝑂(𝑏𝑁2)bits, where𝑏\\nis the bit width of coupling weight – not to mention the effort it\\ntakes to reprogram the Ising machine. In contrast, in our proposed\\nbatch mode, the data volume is 𝑂(𝑁)bits.\\n6 EXPERIMENTAL ANALYSIS\\n6.1 Experimental methodology\\nBecause we are in the early stage of Ising machine development,\\naccess to physical systems is difficult. Most of our comparisons willbe performed with a mixture of modeling, using results reported in\\nliterature, and measuring time of simulated annealing (SA). In all\\nthe experiments, SA is natively executed, while BRIM’s dynamical\\nsystem evolution is modeled by solving differential equations us-\\ning 4th-order Runge-Kutta method. When comparing to reported\\nresults, we are obviously limited by the type of benchmarks that\\nwere used in literature for direct comparison.6Fortunately, a few\\nbenchmarks (K-graphs) have been commonly used. One such graph\\nthat we will use for comparison is known as K16384 [49] and con-\\ntains 16,384 spins with all-to-all connections. Simulating dynamical\\nsystems with differential equations can be orders-of-magnitude\\nslower than cycle-level microprocessor simulation. Simulating 1 𝜇𝑠\\nof dynamics in K16384 takes about 3 days on a very powerful\\nserver. Therefore, we only use it for direct performance compari-\\nson. Smaller K-graphs ( e.g., K2000 [ 28]) are used for some additional\\nanalyses.\\nWhile the execution time of SA is generally the closest thing to\\na standard performance yardstick, there are actually quite some\\nsubtleties. First, different versions have vastly different performance.\\nWe choose Isakov’s algorithm [ 29] as it is the fastest version as\\nfar as we know. Second, we apply an optimization using dense'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 8}, page_content='We choose Isakov’s algorithm [ 29] as it is the fastest version as\\nfar as we know. Second, we apply an optimization using dense\\nmatrix representation. This exploits fully connected graphs like the\\nK-graphs to improve performance. Finally, researchers have tuned\\nthe annealing schedules for these specific graphs. This tuning turns\\nout to have significant impact on execution time. Similar tuning on\\nour hardware annealing schedule could potentially also improve\\nperformance. We can not yet perform such tuning as the simulation\\ncost is prohibitive.\\n6.2 Single solver baseline\\nTo get a sense of the landscape of physical Ising machines and\\ndigital accelerators for simulated annealing, in Fig. 11 we show\\nresults of a few uniprocessor Ising machines running K2000: a\\nBRIM chip (simulated), simulated annealing (measured), and the\\nreported results for STATICA [ 54], CIM [ 28], and two variants of\\nsimulated bifurcation machine (SBM) [ 22]. Many other machines\\nare missing from this comparison because they simply cannot map\\nthe graph. Despite its seemingly modest size (2000 nodes), K2000 is\\na fully connected graph and will take millions of nodes for machines\\nwith only local couplings.\\nFor this graph, BRIM could reach the best known solution of\\n33,337 in 11 𝜇s. The only other machine that could reach similar\\nsolution quality is dSBM in 2 ms, about 180x slower. Even if we are\\nwilling to accept lower solution quality, a single-chip BRIM is still\\nat least an order of magnitude faster.\\nIn summary, a properly designed physical Ising machine can be 6\\norders of magnitude faster than a conventional simulated annealer\\n(SA) and about 2 orders of magnitude faster than the state-of-the-\\nart computational annealer. The only disadvantage of a physical\\nIsing machine over a computational annealer is that the latter can\\nmore easily scale to solve bigger problems. We now look at how\\nthe proposed multiprocessor architecture addresses this issue. We\\nwill narrow our focus to comparing against just SA and SBM [ 49]\\nas the latter is the fastest system at the moment.\\n6Cross-benchmark comparison is full of pitfalls (if not meaningless altogether) as there\\nis no easy way to compare solution quality for different problems.\\n9'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 9}, page_content='ISCA ’22, June 18–22, 2022, New York, NY, USA Anshujit Sharma, et al.\\nFigure 11: Performance of K2000 graph on diverse machines\\nshown as solution cut value on y-axis (higher is better) and\\nexecution time in x-axis. At every time scale, the machine\\ngoes through 100 runs. In machines where multiple time\\nscale results are available, the average results for each time\\nscale are shown in a dashed line and the range is shown as a\\nshaded region. Results with only one time scale are shown as\\nbars with the dots indicating the range and the average cut\\nvalues. Data for bSBM [22], dSBM [22], STATICA [54] and\\nCIM [28] are obtained from the references.\\n6.3 High-level comparison\\nWe compare our proposed multiprocessor BRIM (mBRIM) with SBM\\nusing the larger K16384 graph as the benchmark. This allows us to\\ndirectly compare solution quality and performance with reported\\nresults in the literature. We assume a 4-chip multiprocessor. Each\\nchip is a BRIM-style electronic Ising machine with 8192 nodes.\\nSuch a chip should have a smaller die size (about 80 mm2in a\\n45 nm technology) and consume much less power (less than 10 W)\\nthan a single FPGA used in SBM. We use three incarnations of this\\nmultiprocessor as proxies for different implementation choices:\\n(1)mBRIM 3𝐷: A 3D-integrated version where communication\\nis essentially instantaneous and without bandwidth limit;\\n(2)mBRIM𝐻𝐵: A system with high communication bandwidth.\\nEach chip is provided with three dedicated channel each\\nof 250 GB/s. The total bandwidth is thus close to that of\\nHBM [38].\\n(3)mBRIM𝐿𝐵: A system with low communication bandwidth\\n(4x less than mBRIM 𝐻𝐵).\\nFig. 12 shows the best solution quality and time obtained by dif-\\nferent mBRIMs, by an 8-FPGA implementation of SBM [ 49] and by\\nSA. For clarity, only the results of the best-quality run are shown in\\nthe graph. If we compare the highest performing mBRIM (mBRIM 3𝐷\\nconcurrent mode), with SBM, we see that mBRIM gets to a much\\nbetter solution quality (793,423 to 799,292 vs SBM’s best result of\\nabout 792,000) and is about 2200x faster (1.1 𝜇s vs 2.47 ms). Even\\nthe bandwidth constrained configuration (mBRIM 𝐿𝐵), which we\\nwould operate in batch mode, is more than 700x faster than SBM,\\nalso with a higher solution quality.\\nNext we look at the impact of bandwidth limitation. As already\\ndiscussed in Sec. 4, if the communication bandwidth between chips\\nFigure 12: Solution quality as annealing proceeds for K16384\\ngraph on 4-chip BRIM multiprocessors compared with an 8-\\nFPGA version of SBM [49] and simulated annealing (SA).\\nis insufficient, we can resort to slowing down the Ising machines to\\ncope. The impact, of course, is we need to wait longer to obtain the\\nsolution. Both mBRIM 𝐻𝐵and mBRIM 𝐿𝐵are slower than mBRIM 3𝐷\\ndue to congestion-induced stalling. In these bandwidth limited situ-\\nations, our proposed batch mode operation is a reasonably effective\\ntool and can improve execution speed. Specifically, batch mode\\nallows the same amount of annealing to be finished by 2.8x and 7x\\nfaster for mBRIM 𝐻𝐵and mBRIM 𝐿𝐵, respectively. With batch mode,\\nmBRIM𝐻𝐵is only about 2x slower than mBRIM 3𝐷and mBRIM 𝐿𝐵\\nis another 1.4x slower. However, the solution quality is reduced\\nto 792,728 (which is still better than SBM’s result). We will take a\\nmore detailed look as to how the speed improvement is achieved\\nand other issues in the next section.\\nFinally, we compare mBRIM to SA. We see that to get to the same\\nsolution quality, mBRIM is about 4.5×106faster. This compares to\\nthe 1.3×106speedup in K2000. Note here that there is an extraor-\\ndinary difference (about 140x) for SA’s performance due to tuning\\nthe annealing schedule.\\n6.4 In-depth analysis\\n6.4.1 First-principle approximations. It may be beneficial to under-\\nstand how different types of solver work from first principles. No\\nmatter what solver is used, we need to explore the high-dimensional\\nenergy landscape sufficiently to achieve a good solution. As an'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 9}, page_content='stand how different types of solver work from first principles. No\\nmatter what solver is used, we need to explore the high-dimensional\\nenergy landscape sufficiently to achieve a good solution. As an\\nexample, for the K800 graph (800 nodes, all-to-all), simulated an-\\nnealing (SA) and BRIM explored 148K and 115K different states\\nrespectively to arrive at comparable solution quality. In BRIM, on\\naverage, there is a spin flip every 20ps. In (sequential) SA, flipping\\nindividual spins is achieved computationally: the energy of an al-\\nternative configuration (with a particular spin flipped) is calculated\\nand based on the energy, the new state is probabilistically accepted.\\nRoughly speaking, we count 140,000 instructions executed per spin\\nflip running SA [29].\\n10'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 10}, page_content='Increasing Ising Machine Capacity with Multi-Chip Architectures ISCA ’22, June 18–22, 2022, New York, NY, USA\\nSimulated bifurcation (SB) is an entirely new computational\\napproach. It can be thought of as simulating a dynamical system.\\nThanks to its algorithm design, it is easier to parallelize. Thus,\\ndespite having similar workload, it can be faster.7Nevertheless,\\naccelerating SB to the level of BRIM would require about 1000x\\nmore computational throughput or about 2 Peta Ops per second.\\nWe can see why physical Ising machines are more attractive even\\ncompared to the best computational accelerator.\\n6.4.2 Effects of bandwidth optimizations. As already discussed in\\nSec. 5.4, the degree of global state ignorance can have significant\\nimpact on solution quality. Thus, in concurrent mode, we need to let\\nsolvers frequently update each other. In batch mode, we can tolerate\\nmuch longer epochs and only need to communicate cumulative\\nstate changes between the beginning and end of the same epoch\\n(which we call bit change to differentiate from spin flips). If a spin\\nflips, say, 4 times in an epoch, its state will end up the same as at\\nthe beginning of the epoch, and we do not need to communicate\\nanything. In other words, there are 4 spin flips but 0 bit change.\\nIntuitively, the longer the epoch, the higher the fraction of spin\\nflips will result in no bit change. Fig. 13 confirms this intuition\\nquantitatively.\\nFigure 13: [Left] Evolution of flips and bit changes over\\ntime for a 4 chip BRIM with a fixed epoch size of 3.3 ns.\\nThe left vertical axis corresponds to flips (solid blue line)\\nand bit changes (dashed blue line). The right vertical axis\\ncorresponds to the ratio of flips to bit changes shown in\\nred. [Right] Correlation of the average ratio of flips to bit\\nchanges with epoch size. The ratio increases almost linearly\\nwith increasing epoch size.\\nWe measure the number of spin flips during an epoch and count\\nthe number of bit changes. In Fig. 13 (left), we show both numbers\\nand their ratio as the annealing proceeds. We see that for a fixed\\nepoch size, the ratio is rather stable after an initial period. Fig. 13\\n(right) shows the ratio as a function of different epoch sizes. Not\\nsurprisingly, the longer the epoch the higher the ratio. As we can\\nsee, if we can use an epoch size of about 3 ns, we can reduce traffic\\ndemand by around 4-5x compared to using sub-nanosecond epochs.\\nIncreasing epoch size does degrade solution quality as can be\\nseen in Fig. 14, where we show the solution quality as a function\\nof epoch size. We can see the best solution quality is achieved\\n7A non-trivial portion of SA is also massively parallel. However, there is no effort for\\ncustom-hardware implementation of parallel SA.\\nFigure 14: [Left] Average MaxCut solution for different\\nepochs in concurrent and batch mode. [Right] Magnified y\\naxis.\\nwith concurrent mode with a small epoch size. When bandwidth\\nis sufficient, this is the best mode to use. In a bandwidth-bound\\nsystem, the dynamical system needs to be slowed down. In that\\ncase, 4-5x traffic reduction means the dynamical system can run\\nabout 4-5x faster. Achieving traffic reduction in turn requires longer\\nepochs. In such a case, we see that the concurrent mode does not\\ntolerate longer epochs well and the solution quality drops quickly\\nand significantly. Batch mode, on the other hand, is much more\\ntolerant to longer epochs as the solution quality reduces but only\\nvery slightly. Thus, in a bandwidth-bound system, batch mode will\\nbe very useful in not sacrificing execution speed while maintaining\\nhigh solution quality.\\nFinally, we look at the bandwidth reduction when coordinating\\ninduced spin flips (Sec. 5.4.2). Fig. 15 (left) shows the amount of\\nbit changes and induced spin flips with the evolution of time. The\\npercentage of bit changes due to induced spin flips is also plotted.\\nOf course, the value is a function of epoch size. Fig. 15 (right) shows\\nthe average percentage with different epoch sizes. Clearly a non-'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 10}, page_content='Of course, the value is a function of epoch size. Fig. 15 (right) shows\\nthe average percentage with different epoch sizes. Clearly a non-\\ntrivial amount of communication (30-38%) can be saved with the\\noptimization of coordinating induced flips for both concurrent and\\nbatch modes. In a bandwidth constrained system, a corresponding\\nimprovement in execution time (about 1.5x) can be expected.\\n6.5 Contrast with other parallel processing\\nFinally, we note that communication among distributed agents\\nis clearly a common component and performance bottleneck in\\nparallel processing. Thus, in exploring solutions for Ising machines,\\nwe may have reinvented some wheels. For instance, using shadow\\ncopies is a necessity for us the same way keeping copies (ghosts)\\nof non local neighbors is in parallel algorithms [ 7,20,34,39]. Also,\\ntechniques of reducing communication while limiting performance\\nconsequences have been explored in different contexts: sending\\nlower precision data [ 5] – sometimes just 1 bit [ 10,42], using lossy\\ncompression [ 1], reducing the number of elements transmitted\\n[4,21,44,50,53] or even skipping rounds [ 8,35,43]. Compared to\\nthese situations, two key differences can be highlighted specifically\\nfor the case of Ising machines:\\n(1)Scale of the problem: Ising machines are dynamical systems\\nand can evolve extremely quickly. They thus present enor-\\nmous raw communication demand without optimizations.\\n11'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 11}, page_content='ISCA ’22, June 18–22, 2022, New York, NY, USA Anshujit Sharma, et al.\\nFigure 15: [Left] Evolution of induced spin flips and bit\\nchanges over time for a 4 chip BRIM with a fixed epoch\\nsize of 3.3 ns. The left vertical axis corresponds to induced\\nspin flips (solid blue line) and bit changes (dashed blue line).\\nThe right vertical axis corresponds to the percentage of bit\\nchanges that are induced spin flips (shown in red). [Right]\\nCorrelation of the average percentage of induced spin flips\\nwith epoch size.\\nFor instance our assumed 4 BRIM chips (each about 80 𝑚𝑚2\\nin 45 nm technology) would need about 4 TB/s.\\n(2)Design flexibility: Optimizations such as batch mode and\\ncoordinating induced spin flips are possible because we can\\nexploit the freedom to orchestrate the evolution process\\nof the underlying Ising machine. As a result, there is no\\nadditional logic such as compression and yet we can maintain\\nsolution quality while reducing communication demand to\\nonly 218 GB/s (20x reduction).\\n7 CONCLUSIONS\\nPhysical Ising machines can solve Ising formula optimization prob-\\nlems with extreme speeds and energy efficiencies, even when com-\\npared with special-purpose (von Neumann) accelerators. However,\\nexisting Ising machines have a fixed capacity. If we simply use a\\ndivide-and-conquer strategy, the benefit of using Ising machine\\nreduces quickly when the problem is even slightly bigger than the\\nmachine capacity. Instead we need machines that are fundamen-\\ntally designed to cooperate with other machines to solve a larger\\nproblem. In this paper, we have presented the architectural design\\nand optimizations of a multiprocessor Ising machine . Our analysis\\nof the design can be summarized into a few key takeaway points:\\n(1)Even under conservative conditions we can see that the\\nmultiprocessor can achieve about 2200x speedup compared\\nto the state-of-the-art computational accelerator.\\n(2)With the proposed multiprocessor architecture, a physical\\nIsing machine can now also scale up and solve bigger prob-\\nlems. While it is difficult to compare speedups over different\\nproblems, it is safe to say that the performance advantage of\\na multiprocessor BRIM over its von Neumann counterpart\\nis as significant as in the case of single-chip BRIM.\\n(3)Given the extreme speed of a physical Ising machine, commu-\\nnication bandwidth is likely the performance bottleneck and\\nthe machine dynamics need to slow down correspondingly.\\nIn these cases, our proposed batch mode operation can leadto about 4-5x reduction in communication demand, translat-\\ning to about 4-5x improvement in processing throughput.\\nACKNOWLEDGMENTS\\nThis work has been supported by a University Research Award\\nfrom the University of Rochester, by New York State Center of\\nExcellence in Data Science at the University of Rochester under\\nProject 1689dC13, and also in part by DARPA under Agreement No.\\nHR00112090012.\\nAppendix A DIVIDE-AND-CONQUER\\nALGORITHMS\\nUsers interact with D-Wave systems using the Solver API (SAPI)\\nover network. A job is submitted to a SAPI server queue. Jobs are\\nthen assigned to workers, which run on a conventional proces-\\nsor and are responsible for submitting instructions to the quan-\\ntum processor, receiving results from the quantum processor, post-\\nprocessing results when necessary, and sending results back to\\nuser.\\nAlgorithm 1 D-WAVE’s d-n-c algorithm [13]\\n1:Input:𝑄𝑈𝐵𝑂 instance\\n2:#𝑉𝑏𝑒𝑠𝑡 , lowest value found to date\\n3:#𝑄𝑏𝑒𝑠𝑡 , solution bit vector corresponding to the lowest value so far\\n4:#𝑖𝑛𝑑𝑒𝑥 , indices of the bits in the solution\\n5:\\n6:# Get initial estimate of minimum value and backbone\\n7:𝑄𝑡𝑚𝑝←random 0/1vector\\n8:(𝑉𝑏𝑒𝑠𝑡,𝑄𝑏𝑒𝑠𝑡)←𝑇𝑎𝑏𝑢𝑆𝑒𝑎𝑟𝑐ℎ(𝑄𝑈𝐵𝑂,𝑄𝑡𝑚𝑝)\\n9:𝑖𝑛𝑑𝑒𝑥←𝑂𝑟𝑑𝑒𝑟𝐵𝑦𝐼𝑚𝑝𝑎𝑐𝑡(𝑄𝑈𝐵𝑂,𝑄𝑏𝑒𝑠𝑡)\\n10:𝑝𝑎𝑠𝑠𝐶𝑜𝑢𝑛𝑡←0\\n11:𝑄𝑡𝑚𝑝←𝑄𝑏𝑒𝑠𝑡\\n12:𝑡𝑜𝑡𝑎𝑙←𝑓𝑟𝑎𝑐𝑡𝑖𝑜𝑛∗𝑠𝑖𝑧𝑒(𝑄𝑈𝐵𝑂)\\n13:\\n14:while𝑝𝑎𝑠𝑠𝐶𝑜𝑢𝑛𝑡 <𝑛𝑢𝑚𝑅𝑒𝑝𝑒𝑎𝑡𝑠 do\\n15: for𝑖=0;𝑖<𝑡𝑜𝑡𝑎𝑙 ;𝑖=𝑖+𝑠𝑢𝑏𝑄𝑢𝑏𝑜𝑆𝑖𝑧𝑒 do\\n16: # select subQubo with other variables clamped\\n17:𝑠𝑢𝑏𝑄𝑢𝑏𝑜←𝐶𝑙𝑎𝑚𝑝(𝑄𝑈𝐵𝑂,𝑄𝑡𝑚𝑝,𝑖𝑛𝑑𝑒𝑥 [𝑖:𝑖+𝑠𝑢𝑏𝑄𝑢𝑏𝑜𝑆𝑖𝑧𝑒−1])\\n18:(𝑠𝑢𝑏𝑉,𝑠𝑢𝑏𝑄)←𝐷𝑊𝑎𝑣𝑒𝑆𝑒𝑎𝑟𝑐ℎ(𝑠𝑢𝑏𝑄𝑢𝑏𝑜)\\n19: # project onto full solution'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 11}, page_content='16: # select subQubo with other variables clamped\\n17:𝑠𝑢𝑏𝑄𝑢𝑏𝑜←𝐶𝑙𝑎𝑚𝑝(𝑄𝑈𝐵𝑂,𝑄𝑡𝑚𝑝,𝑖𝑛𝑑𝑒𝑥 [𝑖:𝑖+𝑠𝑢𝑏𝑄𝑢𝑏𝑜𝑆𝑖𝑧𝑒−1])\\n18:(𝑠𝑢𝑏𝑉,𝑠𝑢𝑏𝑄)←𝐷𝑊𝑎𝑣𝑒𝑆𝑒𝑎𝑟𝑐ℎ(𝑠𝑢𝑏𝑄𝑢𝑏𝑜)\\n19: # project onto full solution\\n20:𝑄𝑡𝑚𝑝[𝑖𝑛𝑑𝑒𝑥[𝑖:𝑖+𝑠𝑢𝑏𝑄𝑢𝑏𝑜𝑆𝑖𝑧𝑒−1]]←𝑠𝑢𝑏𝑄\\n21: end for\\n22:(𝑉,𝑄𝑛𝑒𝑤)←𝑇𝑎𝑏𝑢𝑆𝑒𝑎𝑟𝑐ℎ(𝑄𝑈𝐵𝑂,𝑄𝑡𝑚𝑝)\\n23:𝑖𝑛𝑑𝑒𝑥←𝑂𝑟𝑑𝑒𝑟𝐵𝑦𝐼𝑚𝑝𝑎𝑐𝑡(𝑄𝑈𝐵𝑂,𝑄𝑛𝑒𝑤)\\n24: if𝑉<𝑉𝑏𝑒𝑠𝑡 then\\n25:𝑉𝑏𝑒𝑠𝑡←𝑉;𝑄𝑏𝑒𝑠𝑡←𝑄𝑛𝑒𝑤\\n26:𝑝𝑎𝑠𝑠𝐶𝑜𝑢𝑛𝑡←0\\n27: else if𝑉==𝑉𝑏𝑒𝑠𝑡 then\\n28:𝑄𝑏𝑒𝑠𝑡←𝑄𝑛𝑒𝑤\\n29:𝑝𝑎𝑠𝑠𝐶𝑜𝑢𝑛𝑡++\\n30: else\\n31:𝑝𝑎𝑠𝑠𝐶𝑜𝑢𝑛𝑡++\\n32: end if\\n33:𝑄𝑡𝑚𝑝←𝑄𝑛𝑒𝑤\\n34:end while\\n35:Output:𝑉𝑏𝑒𝑠𝑡,𝑄𝑏𝑒𝑠𝑡\\nThe general idea of the algorithm (shown as Algorithm 1 here,\\nreplicated from [ 13]) is straightforward: If part of the state remains\\nfixed, then the original QUBO problem is converted into a smaller\\nsub-problem ( 𝑠𝑢𝑏𝑄𝑢𝑏𝑜 in line 15) that can be launched on a solver\\n(line 18). Repeating this action over different portions of the state\\nvector (lines 15 to 21) constitutes one pass of the algorithm. Multiple\\npasses are performed (while loop starting in line 14) to achieve a\\nbetter result. Shown in Algorithm 2 is our approach which is a bit\\nmore efficient.\\n12'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 12}, page_content='Increasing Ising Machine Capacity with Multi-Chip Architectures ISCA ’22, June 18–22, 2022, New York, NY, USA\\nAlgorithm 2 Our d-n-c algorithm\\n1:Input:𝐺𝑟𝑎𝑝ℎ\\n2:#𝑉←random 0/1spin vector\\n3:#𝑛𝑢𝑚𝑆𝑜𝑙𝑣𝑒𝑟𝑠←number of solvers\\n4:#𝑛𝑢𝑚𝑅𝑒𝑝𝑒𝑎𝑡𝑠←number of times to repeat\\n5:#𝑒𝑝𝑜𝑐ℎ←Epoch times for each solver\\n6:#𝑟𝑎𝑡𝑖𝑜←ratio in which the graph is to be partitioned\\n7:\\n8:(𝑠𝑢𝑏𝐺,𝑠𝑢𝑏𝑉)←𝑅𝑎𝑛𝑑𝑃𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛(𝐺𝑟𝑎𝑝ℎ,𝑉,𝑟𝑎𝑡𝑖𝑜)\\n9:𝑒𝑛𝑒←𝐼𝑠𝑖𝑛𝑔𝐸𝑛𝑒𝑟𝑔𝑦(𝑠𝑢𝑏𝐺,𝑠𝑢𝑏𝑉)\\n10:for𝑟=0;𝑟<𝑛𝑢𝑚𝑅𝑒𝑝𝑒𝑎𝑡𝑠 ;++𝑟do\\n11: for𝑖=0;𝑖<𝑛𝑢𝑚𝑆𝑜𝑙𝑣𝑒𝑟𝑠 ;++𝑖do\\n12: # Launch solvers (Can be parallel)\\n13:𝑆𝑜𝑙𝑣𝑒𝑟(𝑠𝑢𝑏𝐺[𝑖],𝑠𝑢𝑏𝑉[𝑖],𝑒𝑝𝑜𝑐ℎ[𝑖],𝑒𝑛𝑒[𝑖])\\n14: end for\\n15:𝑆𝑦𝑛𝑐ℎ𝑟𝑜𝑛𝑖𝑠𝑒(𝑠𝑢𝑏𝐺,𝑠𝑢𝑏𝑉,𝑒𝑛𝑒)\\n16:end for\\n17:\\n18:𝑉←𝑐𝑜𝑝𝑦(𝑠𝑢𝑏𝑉)\\n19:𝐹𝑖𝑛𝑎𝑙𝐸𝑛𝑒←𝐼𝑠𝑖𝑛𝑔𝐸𝑛𝑒𝑟𝑔𝑦(𝐺𝑟𝑎𝑝ℎ,𝑉)\\n20:Output:𝑉,𝐹𝑖𝑛𝑎𝑙𝐸𝑛𝑒\\nREFERENCES\\n[1]Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig\\nCitro, Gregory S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay\\nGhemawat, Ian J. Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,\\nYangqing Jia, Rafal Józefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg,\\nDan Mané, Rajat Monga, Sherry Moore, Derek Gordon Murray, Chris Olah, Mike\\nSchuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul A.\\nTucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda B. Viégas, Oriol Vinyals,\\nPete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng.\\n2016. TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed\\nSystems. CoRR abs/1603.04467 (2016). arXiv:1603.04467 http://arxiv.org/abs/\\n1603.04467\\n[2]David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. 1985. A learning\\nalgorithm for Boltzmann machines. Cognitive science 9, 1 (1985), 147–169.\\n[3]Richard Afoakwa, Yiqiao Zhang, Uday Kumar Reddy Vengalam, Zeljko Ignjatovic,\\nand Michael Huang. 2021. BRIM: Bistable Resistively-Coupled Ising Machine.\\n2021 IEEE International Symposium on High-Performance Computer Architecture\\n(HPCA) (2021), 749–760. https://doi.org/10.1109/HPCA51647.2021.00068\\n[4]Dan Alistarh, Torsten Hoefler, Mikael Johansson, Sarit Khirirat, Nikola Konstanti-\\nnov, and Cédric Renggli. 2018. The Convergence of Sparsified Gradient Methods.\\nInProceedings of the 32nd International Conference on Neural Information Process-\\ning Systems (Montréal, Canada) (NIPS’18) . Curran Associates Inc., Red Hook, NY,\\nUSA, 5977–5987.\\n[5]Dan Alistarh, Jerry Li, Ryota Tomioka, and Milan Vojnovic. 2016. QSGD: Ran-\\ndomized Quantization for Communication-Optimal Stochastic Gradient Descent.\\nCoRR abs/1610.02132 (2016). arXiv:1610.02132 http://arxiv.org/abs/1610.02132\\n[6]Rami Barends, Alireza Shabani, Lucas Lamata, Julian Kelly, Antonio Mezzacapo,\\nUrtzi Las Heras, Ryan Babbush, Austin G Fowler, Brooks Campbell, Yu Chen, et al .\\n2016. Digitized adiabatic quantum computing with a superconducting circuit.\\nNature 534, 7606 (2016), 222–226.\\n[7]William J. Barry, Mark T. Jones, and Paul E. Plassmann. 1998. Parallel adaptive\\nmesh refinement techniques for plasticity problems. Advances in Engineering\\nSoftware 29, 3 (1998), 217–225. https://doi.org/10.1016/S0965-9978(98)00040-4\\n[8]Debraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi. 2019. Qsparse-Local-\\nSGD: Distributed SGD with Quantization, Sparsification, and Local Computations .\\nCurran Associates Inc., Red Hook, NY, USA.\\n[9]Natalia G Berloff, Matteo Silva, Kirill Kalinin, Alexis Askitopoulos, Julian D\\nTöpfer, Pasquale Cilibrizzi, Wolfgang Langbein, and Pavlos G Lagoudakis. 2017.\\nRealizing the classical XY Hamiltonian in polariton simulators. Nature materials\\n16, 11 (2017), 1120–1126.\\n[10] Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anand-\\nkumar. 2018. signSGD: compressed optimisation for non-convex problems. CoRR\\nabs/1802.04434 (2018). arXiv:1802.04434 http://arxiv.org/abs/1802.04434\\n[11] Fabian Böhm, Guy Verschaffelt, and Guy Van der Sande. 2019. A poor man’s\\ncoherent Ising machine based on opto-electronic feedback systems for solving\\noptimization problems. Nature Communications 10, 1 (2019), 3538. https://doi.\\norg/10.1038/s41467-019-11484-3'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 12}, page_content='coherent Ising machine based on opto-electronic feedback systems for solving\\noptimization problems. Nature Communications 10, 1 (2019), 3538. https://doi.\\norg/10.1038/s41467-019-11484-3\\n[12] Sergio Boixo, Troels F Rønnow, Sergei V Isakov, Zhihui Wang, David Wecker,\\nDaniel A Lidar, John M Martinis, and Matthias Troyer. 2014. Evidence for quantum\\nannealing with more than one hundred qubits. Nature physics 10, 3 (2014), 218–\\n224.\\n[13] Michael Booth, Steven P. Reinhardt, and Aidan Roy. 2017. Partitioning Opti-\\nmization Problems for Hybrid Classical/Quantum Execution. Technical Report\\n(2017). https://docs.ocean.dwavesys.com/projects/qbsolv/en/latest/_downloads/\\nbd15a2d8f32e587e9e5997ce9d5512cc/qbsolv_techReport.pdf[14] Paul I Bunyk, Emile M Hoskinson, Mark W Johnson, Elena Tolkacheva, Fabio\\nAltomare, Andrew J Berkley, Richard Harris, Jeremy P Hilton, Trevor Lanting,\\nAnthony J Przybysz, et al .2014. Architectural considerations in the design of\\na superconducting quantum annealing processor. IEEE Transactions on Applied\\nSuperconductivity 24, 4 (2014), 1–10.\\n[15] KyungHyun Cho, Alexander Ilin, and Tapani Raiko. 2011. Improved learning of\\nGaussian-Bernoulli restricted Boltzmann machines. In International conference\\non artificial neural networks . Springer, 10–17.\\n[16] Jeffrey Chou, Suraj Bramhavar, Siddhartha Ghosh, and William Herzog. 2019.\\nAnalog Coupled Oscillator Based Weighted Ising Machine. Scientific Reports 9, 1\\n(2019), 14786. https://doi.org/10.1038/s41598-019-49699-5\\n[17] Chase Cook, Hengyang Zhao, Takashi Sato, Masayuki Hiromoto, and Sheldon\\nX. D. Tan. 2019. GPU Based Parallel Ising Computing for Combinatorial Opti-\\nmization Problems in VLSI Physical Design. arXiv:1807.10750 [physics.comp-ph]\\n[18] D-WAVE. 2014. minorminer. https://github.com/dwavesystems/minorminer\\n[19] D-WAVE. 2022. Operation and Timing. https://docs.dwavesys.com/docs/latest/\\nc_qpu_timing.html\\n[20] Zachary DeVito, Niels Joubert, Francisco Palacios, Stephen Oakley, Montserrat\\nMedina, Mike Barrientos, Erich Elsen, Frank Ham, Alex Aiken, Karthik Duraisamy,\\nEric Darve, Juan Alonso, and Pat Hanrahan. 2011. Liszt: A domain specific\\nlanguage for building portable mesh-based PDE solvers. In SC ’11: Proceedings\\nof 2011 International Conference for High Performance Computing, Networking,\\nStorage and Analysis . 1–12. https://doi.org/10.1145/2063384.2063396\\n[21] Melih Elibol, Lihua Lei, and Michael I. Jordan. 2020. Variance Reduction with\\nSparse Gradients. CoRR abs/2001.09623 (2020). arXiv:2001.09623 https://arxiv.\\norg/abs/2001.09623\\n[22] Hayato Goto, Kotaro Endo, Masaru Suzuki, Yoshisato Sakai, Taro Kanao, Yohei\\nHamakawa, Ryo Hidaka, Masaya Yamasaki, and Kosuke Tatsumura. 2021. High-\\nperformance combinatorial optimization based on classical mechanics. Sci-\\nence Advances 7, 6 (2021), eabe7953. https://doi.org/10.1126/sciadv.abe7953\\narXiv:https://www.science.org/doi/pdf/10.1126/sciadv.abe7953\\n[23] Hidenori GYOTEN, Masayuki HIROMOTO, and Takashi SATO. 2018. Area\\nEfficient Annealing Processor for Ising Model without Random Number Gener-\\nator. IEICE Transactions on Information and Systems E101.D, 2 (2018), 314–323.\\nhttps://doi.org/10.1587/transinf.2017RCP0015\\n[24] Ryan Hamerly, Takahiro Inagaki, Peter L McMahon, Davide Venturelli, Alireza\\nMarandi, Tatsuhiro Onodera, Edwin Ng, Carsten Langrock, Kensuke Inaba, et al .\\n2018. Scaling advantages of all-to-all connectivity in physical annealers: the\\nCoherent Ising Machine vs. D-Wave 2000Q. D-Wave 2000Q arXiv (2018).\\n[25] Ryan Hamerly, Takahiro Inagaki, Peter L McMahon, Davide Venturelli, Alireza\\nMarandi, Tatsuhiro Onodera, Edwin Ng, Carsten Langrock, Kensuke Inaba, Toshi-\\nmori Honjo, et al .2019. Experimental investigation of performance differences\\nbetween coherent Ising machines and a quantum annealer. Science advances 5, 5\\n(2019), eaau0823.\\n[26] R Hamerly, A Sludds, L Bernstein, M Prabhu, C Roques-Carmes, J Carolan, Y\\nYamamoto, M Soljačić, and D Englund. 2019. Towards Large-Scale Photonic'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 12}, page_content='(2019), eaau0823.\\n[26] R Hamerly, A Sludds, L Bernstein, M Prabhu, C Roques-Carmes, J Carolan, Y\\nYamamoto, M Soljačić, and D Englund. 2019. Towards Large-Scale Photonic\\nNeural-Network Accelerators. In 2019 IEEE International Electron Devices Meeting\\n(IEDM) . IEEE, 22–8.\\n[27] R. Harris, M. W. Johnson, T. Lanting, A. J. Berkley, J. Johansson, P. Bunyk, E.\\nTolkacheva, E. Ladizinsky, N. Ladizinsky, T. Oh, F. Cioata, I. Perminov, P. Spear,\\nC. Enderud, C. Rich, S. Uchaikin, M. C. Thom, E. M. Chapple, J. Wang, B. Wilson,\\nM. H. S. Amin, N. Dickson, K. Karimi, B. Macready, C. J. S. Truncik, and G. Rose.\\n2010. Experimental investigation of an eight-qubit unit cell in a superconducting\\noptimization processor. Phys. Rev. B 82 (Jul 2010), 024511. Issue 2. https://doi.\\norg/10.1103/PhysRevB.82.024511\\n[28] Takahiro Inagaki, Yoshitaka Haribara, Koji Igarashi, Tomohiro Sonobe, Shuhei\\nTamate, Toshimori Honjo, Alireza Marandi, Peter L McMahon, Takeshi Umeki,\\nKoji Enbutsu, et al .2016. A coherent Ising machine for 2000-node optimization\\nproblems. Science 354, 6312 (2016), 603–606.\\n[29] S.V. Isakov, I.N. Zintchenko, T.F. Rønnow, and M. Troyer. 2015. Optimised simu-\\nlated annealing for Ising spin glasses. Computer Physics Communications 192 (Jul\\n2015), 265–271. https://doi.org/10.1016/j.cpc.2015.02.015\\n[30] Richard M. Karp. 1972. Reducibility among Combinatorial Problems . Springer US,\\nBoston, MA, 85–103. https://doi.org/10.1007/978-1-4684-2001-2_9\\n[31] Kihwan Kim, M-S Chang, Simcha Korenblit, Rajibul Islam, Emily E Edwards,\\nJames K Freericks, G-D Lin, L-M Duan, and Christopher Monroe. 2010. Quantum\\nsimulation of frustrated Ising spins with trapped ions. Nature 465, 7298 (2010),\\n590–593.\\n[32] Andrew D King, Juan Carrasquilla, Jack Raymond, Isil Ozfidan, Evgeny Andriyash,\\nAndrew Berkley, Mauricio Reis, Trevor Lanting, Richard Harris, Fabio Altomare,\\net al.2018. Observation of topological phenomena in a programmable lattice of\\n1,800 qubits. Nature 560, 7719 (2018), 456–460.\\n[33] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. 1983. Optimization by Simulated\\nAnnealing. Science 220, 4598 (1983), 671–680. https://doi.org/10.1126/science.\\n220.4598.671 arXiv:https://science.sciencemag.org/content/220/4598/671.full.pdf\\n[34] Orion S. Lawlor, Sayantan Chakravorty, Terry L. Wilmarth, Nilesh Choudhury,\\nIsaac Dooley, Gengbin Zheng, and Laxmikant V. Kalé. 2006. ParFUM: A Parallel\\nFramework for Unstructured Meshes for Scalable Dynamic Physics Applications.\\n13'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 13}, page_content='ISCA ’22, June 18–22, 2022, New York, NY, USA Anshujit Sharma, et al.\\nEng. with Comput. 22, 3 (dec 2006), 215–235.\\n[35] Tao Lin, Sebastian U. Stich, and Martin Jaggi. 2018. Don’t Use Large Mini-\\nBatches, Use Local SGD. CoRR abs/1808.07217 (2018). arXiv:1808.07217 http:\\n//arxiv.org/abs/1808.07217\\n[36] Andrew Lucas. 2014. Ising formulations of many NP problems. Frontiers in\\nPhysics 2 (2014), 5.\\n[37] Peter L. McMahon, Alireza Marandi, Yoshitaka Haribara, Ryan Hamerly, Carsten\\nLangrock, Shuhei Tamate, Takahiro Inagaki, Hiroki Takesue, Shoko Utsunomiya,\\nKazuyuki Aihara, Robert L. Byer, M. M. Fejer, Hideo Mabuchi, and Yoshihisa Ya-\\nmamoto. 2016. A fully programmable 100-spin coherent Ising machine with all-to-\\nall connections. Science 354, 6312 (2016), 614–617. https://doi.org/10.1126/science.\\naah5178 arXiv:https://science.sciencemag.org/content/354/6312/614.full.pdf\\n[38] Chris Mellor. 2021. DRAM, it stacks up: SK hynix rolls out 819 GB/s HBM3 tech.\\nhttps://www.theregister.com/2021/10/20/sk_hynix_hbm3/\\n[39] Misbah Mubarak, Seegyoung Seol, Qiukai Lu, and Mark S. Shephard. 1900. A Par-\\nallel Ghosting Algorithm for The Flexible Distributed Mesh Database. Scientific\\nProgramming 21 (01 Jan 1900), 654971. https://doi.org/10.3233/SPR-130361\\n[40] Saavan Patel, Lili Chen, Philip Canoza, and Sayeef Salahuddin. 2020. Ising Model\\nOptimization Problems on a FPGA Accelerated Restricted Boltzmann Machine.\\narXiv:2008.04436 [cs.AR]\\n[41] D Pierangeli, G Marcucci, and C Conti. 2019. Large-scale photonic Ising machine\\nby spatial light modulation. Physical review letters 122, 21 (2019), 213902.\\n[42] Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 2014. 1-Bit Stochastic\\nGradient Descent and Application to Data-Parallel Distributed Training of Speech\\nDNNs. In Interspeech 2014 (interspeech 2014 ed.). https://www.microsoft.com/en-\\nus/research/publication/1-bit-stochastic-gradient-descent-and-application-to-\\ndata-parallel-distributed-training-of-speech-dnns/\\n[43] Sebastian U. Stich. 2019. Local SGD Converges Fast and Communicates Little. In\\nInternational Conference on Learning Representations . https://openreview.net/\\nforum?id=S1g2JnRcFX\\n[44] Sebastian U. Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. 2018. Sparsified\\nSGD with Memory. In Proceedings of the 32nd International Conference on Neural\\nInformation Processing Systems (Montréal, Canada) (NIPS’18) . Curran Associates\\nInc., Red Hook, NY, USA, 4452–4463.\\n[45] Kenta Takata, Alireza Marandi, Ryan Hamerly, Yoshitaka Haribara, Daiki Maruo,\\nShuhei Tamate, Hiromasa Sakaguchi, Shoko Utsunomiya, and Yoshihisa Ya-\\nmamoto. 2016. A 16-bit Coherent Ising Machine for One-Dimensional Ring\\nand Cubic Graph Problems. Scientific Reports 6, 1 (2016), 34089. https://doi.org/\\n10.1038/srep34089\\n[46] Y Takeda, S Tamate, Y Yamamoto, H Takesue, T Inagaki, and S Utsunomiya. 2017.\\nBoltzmann sampling for an XY model using a non-degenerate optical parametric\\noscillator network. Quantum Science and Technology 3, 1 (nov 2017), 014004.\\nhttps://doi.org/10.1088/2058-9565/aa923b\\n[47] T. Takemoto, M. Hayashi, C. Yoshimura, and M. Yamaoka. 2019. 2.6 A 2 by 30k-\\nSpin Multichip Scalable Annealing Processor Based on a Processing-In-Memory\\nApproach for Solving Large-Scale Combinatorial Optimization Problems. In IEEE\\nInternational Solid- State Circuits Conference .\\n[48] Kosuke Tatsumura, Alexander R. Dixon, and Hayato Goto. 2019. FPGA-Based\\nSimulated Bifurcation Machine. In 2019 29th International Conference on Field\\nProgrammable Logic and Applications (FPL) . 59–66. https://doi.org/10.1109/FPL.\\n2019.00019\\n[49] Kosuke Tatsumura, Masaya Yamasaki, and Hayato Goto. 2021. Scaling out Ising\\nmachines using a multi-chip architecture for simulated bifurcation. Nature\\nElectronics 4, 3 (01 Mar 2021), 208–217. https://doi.org/10.1038/s41928-021-\\n00546-4\\n[50] Hongyi Wang, Scott Sievert, Zachary Charles, Shengchao Liu, Stephen Wright,\\nand Dimitris Papailiopoulos. 2018. ATOMO: Communication-Efficient Learning'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 13}, page_content='00546-4\\n[50] Hongyi Wang, Scott Sievert, Zachary Charles, Shengchao Liu, Stephen Wright,\\nand Dimitris Papailiopoulos. 2018. ATOMO: Communication-Efficient Learning\\nvia Atomic Sparsification. In Proceedings of the 32nd International Conference\\non Neural Information Processing Systems (Montréal, Canada) (NIPS’18) . Curran\\nAssociates Inc., Red Hook, NY, USA, 9872–9883.\\n[51] Tianshi Wang and Jaijeet Roychowdhury. 2019. OIM: Oscillator-Based\\nIsing Machines for Solving Combinatorial Optimisation Problems.\\narXiv:1903.07163 [cs.ET]\\n[52] Tianshi Wang, Leon Wu, and Jaijeet Roychowdhury. 2019. New Computational\\nResults and Hardware Prototypes for Oscillator-Based Ising Machines. In Pro-\\nceedings of the 56th Annual Design Automation Conference 2019 (Las Vegas, NV,\\nUSA) (DAC ’19) . Association for Computing Machinery, New York, NY, USA,\\nArticle 239, 2 pages. https://doi.org/10.1145/3316781.3322473\\n[53] Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang. 2018. Gradi-\\nent Sparsification for Communication-Efficient Distributed Optimization. In\\nAdvances in Neural Information Processing Systems , S. Bengio, H. Wal-\\nlach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.),\\nVol. 31. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2018/file/\\n3328bdf9a4b9504b9398284244fe97c2-Paper.pdf[54] Kasho Yamamoto, Kazushi Kawamura, Kota Ando, Normann Mertig, Takashi\\nTakemoto, Masanao Yamaoka, Hiroshi Teramoto, Akira Sakai, Shinya Takamaeda-\\nYamazaki, and Masato Motomura. 2021. STATICA: A 512-Spin 0.25M-Weight An-\\nnealing Processor With an All-Spin-Updates-at-Once Architecture for Combina-\\ntorial Optimization With Complete Spin–Spin Interactions. IEEE Journal of Solid-\\nState Circuits 56, 1 (2021), 165–178. https://doi.org/10.1109/JSSC.2020.3027702\\n[55] Yoshihisa Yamamoto, Kazuyuki Aihara, Timothee Leleu, Ken-ichi Kawarabayashi,\\nSatoshi Kako, Martin Fejer, Kyo Inoue, and Hiroki Takesue. 2017. Coherent Ising\\nmachines—Optical neural networks operating at the quantum limit. npj Quantum\\nInformation 3, 1 (2017), 1–15.\\n[56] Masanao Yamaoka, Chihiro Yoshimura, Masato Hayashi, Takuya Okuyama, Hide-\\ntaka Aoki, and Hiroyuki Mizuno. 2015. A 20k-spin Ising chip to solve combina-\\ntorial optimization problems with CMOS annealing. IEEE Journal of Solid-State\\nCircuits 51, 1 (2015), 303–309.\\n[57] M. Yamaoka, C. Yoshimura, M. Hayashi, T. Okuyama, H. Aoki, and H. Mizuno.\\n2015. 24.3 20k-spin Ising chip for combinational optimization problem with CMOS\\nannealing. In 2015 IEEE International Solid-State Circuits Conference - (ISSCC)\\nDigest of Technical Papers . 1–3. https://doi.org/10.1109/ISSCC.2015.7063111\\n[58] Chihiro Yoshimura, Masato Hayashi, Takuya Okuyama, and Masanao Yamaoka.\\n2017. Implementation and Evaluation of FPGA-based Annealing Processor for\\nIsing Model by use of Resource Sharing. International Journal of Networking\\nand Computing 7, 2 (2017), 154–172. http://www.ijnc.org/index.php/ijnc/article/\\nview/148\\n[59] G Zames, NM Ajlouni, NM Ajlouni, NM Ajlouni, JH Holland, WD Hills, and DE\\nGoldberg. 1981. Genetic algorithms in search, optimization and machine learning.\\nInformation Technology Journal 3, 1 (1981), 301–302.\\n14')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"data/isca22.pdf\")\n",
    "pages  = loader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the question based on the context below. If you can't\n",
      "answer the question, reply \"I don't know\"\n",
      "\n",
      "Context: Here is some context\n",
      "Question: Here is a question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't\n",
    "answer the question, reply \"I don't know\"\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(prompt.format(context=\"Here is some context\", question=\"Here is a question\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'context': {'title': 'Context', 'type': 'string'},\n",
       "  'question': {'title': 'Question', 'type': 'string'}},\n",
       " 'required': ['context', 'question']}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Anshujit'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"context\" : \"The name I was given was Anshujit\",\n",
    "        \"question\": \"What is my name?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(\n",
    "    pages,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/isca22.pdf', 'page': 13}, page_content='00546-4\\n[50] Hongyi Wang, Scott Sievert, Zachary Charles, Shengchao Liu, Stephen Wright,\\nand Dimitris Papailiopoulos. 2018. ATOMO: Communication-Efficient Learning\\nvia Atomic Sparsification. In Proceedings of the 32nd International Conference\\non Neural Information Processing Systems (Montréal, Canada) (NIPS’18) . Curran\\nAssociates Inc., Red Hook, NY, USA, 9872–9883.\\n[51] Tianshi Wang and Jaijeet Roychowdhury. 2019. OIM: Oscillator-Based\\nIsing Machines for Solving Combinatorial Optimisation Problems.\\narXiv:1903.07163 [cs.ET]\\n[52] Tianshi Wang, Leon Wu, and Jaijeet Roychowdhury. 2019. New Computational\\nResults and Hardware Prototypes for Oscillator-Based Ising Machines. In Pro-\\nceedings of the 56th Annual Design Automation Conference 2019 (Las Vegas, NV,\\nUSA) (DAC ’19) . Association for Computing Machinery, New York, NY, USA,\\nArticle 239, 2 pages. https://doi.org/10.1145/3316781.3322473\\n[53] Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang. 2018. Gradi-\\nent Sparsification for Communication-Efficient Distributed Optimization. In\\nAdvances in Neural Information Processing Systems , S. Bengio, H. Wal-\\nlach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.),\\nVol. 31. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2018/file/\\n3328bdf9a4b9504b9398284244fe97c2-Paper.pdf[54] Kasho Yamamoto, Kazushi Kawamura, Kota Ando, Normann Mertig, Takashi\\nTakemoto, Masanao Yamaoka, Hiroshi Teramoto, Akira Sakai, Shinya Takamaeda-\\nYamazaki, and Masato Motomura. 2021. STATICA: A 512-Spin 0.25M-Weight An-\\nnealing Processor With an All-Spin-Updates-at-Once Architecture for Combina-\\ntorial Optimization With Complete Spin–Spin Interactions. IEEE Journal of Solid-\\nState Circuits 56, 1 (2021), 165–178. https://doi.org/10.1109/JSSC.2020.3027702\\n[55] Yoshihisa Yamamoto, Kazuyuki Aihara, Timothee Leleu, Ken-ichi Kawarabayashi,\\nSatoshi Kako, Martin Fejer, Kyo Inoue, and Hiroki Takesue. 2017. Coherent Ising\\nmachines—Optical neural networks operating at the quantum limit. npj Quantum\\nInformation 3, 1 (2017), 1–15.\\n[56] Masanao Yamaoka, Chihiro Yoshimura, Masato Hayashi, Takuya Okuyama, Hide-\\ntaka Aoki, and Hiroyuki Mizuno. 2015. A 20k-spin Ising chip to solve combina-\\ntorial optimization problems with CMOS annealing. IEEE Journal of Solid-State\\nCircuits 51, 1 (2015), 303–309.\\n[57] M. Yamaoka, C. Yoshimura, M. Hayashi, T. Okuyama, H. Aoki, and H. Mizuno.\\n2015. 24.3 20k-spin Ising chip for combinational optimization problem with CMOS\\nannealing. In 2015 IEEE International Solid-State Circuits Conference - (ISSCC)\\nDigest of Technical Papers . 1–3. https://doi.org/10.1109/ISSCC.2015.7063111\\n[58] Chihiro Yoshimura, Masato Hayashi, Takuya Okuyama, and Masanao Yamaoka.\\n2017. Implementation and Evaluation of FPGA-based Annealing Processor for\\nIsing Model by use of Resource Sharing. International Journal of Networking\\nand Computing 7, 2 (2017), 154–172. http://www.ijnc.org/index.php/ijnc/article/\\nview/148\\n[59] G Zames, NM Ajlouni, NM Ajlouni, NM Ajlouni, JH Holland, WD Hills, and DE\\nGoldberg. 1981. Genetic algorithms in search, optimization and machine learning.\\nInformation Technology Journal 3, 1 (1981), 301–302.\\n14'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 7}, page_content='induced spin flips.) These spin flips are generally applied stochasti-\\ncally, similar to an accepted proposal in the Metropolis algorithm.\\nIn a practical implementation, randomness is often of a determinis-\\ntic, pseudo-random nature. As a result, if we properly synchronize\\nthe pseudo random number generator (PRNG) on each chip, we\\ncan guarantee that each chip will generate the same output every-\\nwhere at about the same time. In this way, we can apply induced\\nspin flips without explicit communication. In other words, instead\\nof randomly choosing, say, spin node 3 to flip and then sending\\nan explicit message from the chip that contains the node to other\\nchips informing them of the flip, the PRNG on all chips would be\\nprogrammed to induce node 3 (and its shadow copies) to flip and\\nupdate the nodal capacitor or the shadow register at about the same\\ntime.\\nTo sum: phenomenologically multiple solvers canoperate con-\\ncurrently and seems to offer the highest solution quality, provided\\nthey keep each other informed “sufficiently promptly”. This means,\\nwe have to choose a short epoch time, which generally means high\\ncommunication demands. One opportunity to reduce the demand\\nis using properly synchronized PRNG for induced spin flips.\\n5.5 Batch mode operation\\nWhile careful design of concurrent operation can achieve noticeable\\nbandwidth savings (about 1.5x as we shall see), a completely differ-\\nent mode of operation – batch mode – can allow a more substantial\\nsavings (about 5x). This mode leverages the fact that a common, if\\nnot universal, mode of using an annealer is to perform a batch of\\nannealing jobs of the same problem with different initial states and\\ntake the best solution from the batch. Each job is thus independent\\nof each other. Knowing that we have a batch of jobs of the same\\nsetup , we can stagger them in a fairly straightforward manner to\\nreduce the necessary communication.\\nThe key idea is illustrated in Fig. 10. Viewed vertically, a single\\njob (from one initial state, say Job 1) is still spread out over multiple\\nsolvers (on Chip 1 in epoch 1, on Chip 2 in epoch 2, etc.) like in\\nconcurrent mode. So each chip is still just annealing for its part of\\n8'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 10}, page_content='Of course, the value is a function of epoch size. Fig. 15 (right) shows\\nthe average percentage with different epoch sizes. Clearly a non-\\ntrivial amount of communication (30-38%) can be saved with the\\noptimization of coordinating induced flips for both concurrent and\\nbatch modes. In a bandwidth constrained system, a corresponding\\nimprovement in execution time (about 1.5x) can be expected.\\n6.5 Contrast with other parallel processing\\nFinally, we note that communication among distributed agents\\nis clearly a common component and performance bottleneck in\\nparallel processing. Thus, in exploring solutions for Ising machines,\\nwe may have reinvented some wheels. For instance, using shadow\\ncopies is a necessity for us the same way keeping copies (ghosts)\\nof non local neighbors is in parallel algorithms [ 7,20,34,39]. Also,\\ntechniques of reducing communication while limiting performance\\nconsequences have been explored in different contexts: sending\\nlower precision data [ 5] – sometimes just 1 bit [ 10,42], using lossy\\ncompression [ 1], reducing the number of elements transmitted\\n[4,21,44,50,53] or even skipping rounds [ 8,35,43]. Compared to\\nthese situations, two key differences can be highlighted specifically\\nfor the case of Ising machines:\\n(1)Scale of the problem: Ising machines are dynamical systems\\nand can evolve extremely quickly. They thus present enor-\\nmous raw communication demand without optimizations.\\n11'),\n",
       " Document(metadata={'source': 'data/isca22.pdf', 'page': 6}, page_content='connected by TSV).\\nFinally, we can always slow down the physics of the Ising ma-\\nchine so that the communication demand matches the supply of\\nthe fabric. In the case of BRIM, this can be achieved in a combina-\\ntion of (at least) two ways. First, the machine’s RC constant can be\\nincreased – higher coupling resistors will be used to slow down\\ncharging. Second, the system can be stopped altogether, for in-\\nstance, to wait out a congestion. No matter how we combine these\\nmechanisms the math is simple: to reduce bandwidth demand by 2x,\\nwe need to slow down the machine by 2x. There are things that we\\ncan do to reduce the bandwidth demand without a corresponding\\nreduction in performance. We discuss these next.\\n5.4 Concurrent mode operation\\nConcurrent operation of multiple Ising machines (solving the same\\nproblem) can be roughly described as a combination of each ma-\\nchine performing local searches independently and exchanging\\ninformation about the state of spins with each other. A surpris-\\ningly consequential design parameter is how long to wait before\\ncommunicating a change of spin to others. Sending any change\\nimmediately seems the natural choice as the multiprocessor func-\\ntions most closely to a monolithic, large physical Ising machine.\\nHowever, waiting has its merit too. During a window of time, a\\nspin may flip back and forth multiple times. With some waiting, we\\navoid wasting bandwidth on unnecessary updates. In this regards,\\n7')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retriever can be from google search for example\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# return the 4 topmost relevant documents for the given concept\n",
    "retriever.invoke(\"Ising machine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ising machines use magnetic fields and spins on atoms that can either align with or against the field.\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\" : itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "print(chain.invoke({\"question\" : \"What are Ising machines?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How does Ising machine solve QUBO problem?\n",
      "Answer: \n",
      "\n",
      "Question: What is the motivation of the paper?\n",
      "Answer: The authors discuss how different types of solvers work from first principles and explore the high-dimensional energy landscape sufficiently to achieve a good solution. They use simulated annealing (SA) and BRIM as examples, both exploring 148K and 115K different states respectively to arrive at comparable solution quality.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"How does Ising machine solve QUBO problem?\",\n",
    "    \"What is the motivation of the paper?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    answer = chain.invoke({\"question\" : question})\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The problem the paper aims to solve can be described as optimizing a system represented by an objective function, where some variables have to satisfy certain constraints."
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"question\" : \"What is the problem that the paper is trying to solve?\"}):\n",
    "    print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Previous Ising machine implementations have faced several challenges and limitations, including:\\n\\n1. Memory constraints: Many previous Ising machine implementations were limited by the available memory on a single chip or device, which hindered their ability to solve large-scale optimization problems.\\n2. Computational complexity: The computational complexity of traditional Ising machine algorithms can be high, particularly for large problem sizes, which can limit their scalability.\\n3. Communication overhead: In distributed computing settings, the communication overhead between processors can be significant, especially when using traditional parallelization strategies.\\n4. Limited parallelism: Traditional Ising machine implementations often have limited parallelism, which can result in slower convergence rates and reduced scalability.\\n5. Non-optimal problem solving: Some previous Ising machine implementations may not always find the global optimum for a given problem, which can lead to suboptimal solutions or increased computational time.\\n6. Limited flexibility: Traditional Ising machines are often designed for specific problem types and may not be easily adaptable to other optimization problems.\\n7. Lack of parallelism in the optimization process: Many previous Ising machine implementations use a sequential optimization approach, which can lead to limited parallelism and slower convergence rates.\\n8. Limited applicability: Traditional Ising machines are primarily designed for solving combinatorial optimization problems, but may not be directly applicable to other types of problems.\\n9. Difficulty in scaling to large problem sizes: As the size of the problem increases, the computational complexity of traditional Ising machine algorithms can become prohibitively high, limiting their ability to solve large-scale optimization problems.\\n10. Limited control over the optimization process: Traditional Ising machines often lack fine-grained control over the optimization process, which can lead to suboptimal solutions or increased computational time.',\n",
       " 'Based on the provided references, BRIM (pronounced \"brim\") appears to be a new research field that combines ideas from machine learning and classical optimization to solve complex problems more efficiently. The name \"BRIM\" is an acronym for \"Bit-level Reconfigurable Ising Machines,\" which refers to the fact that these machines are built using bits (binary digits) and can be reconfigured to perform different optimization tasks.\\n\\nThe concept of BRIM was first introduced in a research paper by Frank Seide, et al., in 2014 [1]. The authors proposed a new algorithm called 1-bit stochastic gradient descent (1-BSGD), which uses a single bit to represent the state of a variable during optimization. This approach allows for faster convergence and reduced communication overhead compared to traditional distributed training methods.\\n\\nSince then, there have been several research papers published on BRIM, exploring its applications in various fields such as machine learning [2-4], combinatorial optimization [5], and quantum computing [6]. These works have shown that BRIM can be used to solve complex problems more efficiently than traditional methods, often with lower communication overhead.\\n\\nOverall, BRIM appears to be a promising research field that combines the advantages of machine learning and classical optimization techniques to solve complex problems in various domains.',\n",
       " 'The main motivation behind this paper appears to be the development of a novel optimization algorithm, specifically designed for solving combinatorial optimization problems using stochastic gradient descent (SGD) with a memory-efficient approach. The authors aim to improve the efficiency and scalability of SGD in distributed computing environments by reducing the communication overhead between processors.\\n\\nTo achieve this goal, the paper proposes several techniques:\\n\\n1. 1-Bit Stochastic Gradient Descent (SGD): The authors propose an efficient variant of SGD called 1-bit SGD, which reduces the precision of the gradient values from 32 bits to 1 bit. This leads to significant memory savings and faster convergence rates.\\n2. Application to Data-Parallel Distributed Training of Speech DNNs: The authors apply their proposed algorithm to a large-scale speech recognition task using data parallelism, which enables them to train deeper models with more data samples.\\n3. Local SGD Converges Fast and Communicates Little: The authors propose a local SGD variant that reduces the communication overhead by processing gradient updates locally within each processor before sharing them across processors. This results in faster convergence rates while maintaining a small memory footprint.\\n4. Boltzmann Sampling for an XY Model using a Non-Degenerate Optical Parametric Oscillator Network: The authors propose a novel application of SGD to the Ising model, which is a fundamental problem in statistical physics. They use a non-degenerate optical parametric oscillator network to perform Boltzmann sampling for the XY model, demonstrating the versatility of their approach.\\n5. FPGA-Based Simulated Bifurcation Machine: The authors propose a hardware accelerator based on field-programmable gate arrays (FPGAs) to efficiently simulate the bifurcation machine. This enables faster and more efficient solution of combinatorial optimization problems in large-scale computing environments.\\n6. Scaling out Ising Machines using a Multi-Chip Architecture: The authors propose a multi-chip architecture for scaling out Ising machines, which are used to solve combinatorial optimization problems. This leads to improved efficiency and scalability of the machine while maintaining accuracy.\\n\\nIn summary, the main motivation behind this paper is to develop novel optimization algorithms that can efficiently solve large-scale combinatorial optimization problems using stochastic gradient descent (SGD) with a memory-efficient approach in distributed computing environments. The proposed techniques aim to reduce communication overheads and improve scalability while maintaining accuracy.',\n",
       " 'The paper proposes several solutions to address the issue of slow convergence and high communication overhead in traditional stochastic gradient descent (SGD) algorithms, particularly when applied to large-scale machine learning tasks. Here are some key proposals:\\n\\n1. 1-bit SGD: The authors introduce a novel variant of SGD called 1-bit SGD, which reduces the communication overhead by representing each gradient entry as a single bit. This leads to faster convergence and reduced communication costs.\\n2. Stochastic gradient descent with memory (SGM): The paper proposes a new algorithm called SGM, which stores a small set of recent gradients in memory and uses them to compute the next step. This reduces the number of communication rounds required for convergence.\\n3. Sparsified SGD: The authors propose a variant of SGD that adaptively reduces the number of non-zero coefficients in each gradient update, based on their magnitude. This leads to faster convergence and reduced communication costs.\\n4. Boltzmann sampling for XY models using optical parametric oscillator networks (OPONs): The paper proposes an optimization algorithm called Boltzmann sampling, which uses a non-degenerate OPON to generate samples from the Boltzmann distribution of the XY model. This can be used to solve large-scale combinatorial optimization problems with low computational complexity.\\n5. FPGA-based simulated bifurcation machine: The authors propose an FPGA-based hardware accelerator for solving large-scale bifurcation problems using a simulated bifurcation machine. This can significantly reduce the computational time required for simulation and optimization.\\n6. Multi-chip architecture for Ising machines: The paper proposes a multi-chip architecture for Ising machines, which can scale out the computation of the Ising model by distributing it across multiple chips. This leads to faster convergence and larger problem sizes that can be solved.',\n",
       " 'The proposed idea of using a non-degenerate optical parametric oscillator (OPO) network for simulated bifurcation machines is novel and has the potential to offer significant improvements over existing proposals. Here are some ways in which the proposed idea can be better than existing proposals:\\n\\n1. Scalability: The proposed OPO-based machine can scale up to larger problem sizes by simply adding more OPOs, whereas existing proposals may require more complex and expensive hardware configurations to achieve the same level of scalability.\\n2. Speed: The use of non-degenerate OPOs can result in faster convergence rates compared to traditional annealing machines, which can lead to faster solution times for combinatorial optimization problems.\\n3. Flexibility: The proposed machine can be easily adapted to different problem types and sizes by changing the OPO network architecture, whereas existing proposals may be more limited in their adaptability.\\n4. Noise tolerance: The use of non-degenerate OPOs can result in higher noise tolerance compared to traditional annealing machines, which can be beneficial for solving problems with noisy or incomplete information.\\n5. Energy efficiency: The proposed machine has the potential to be more energy-efficient compared to existing proposals, as it uses a photonic architecture that can operate at lower powers and generate less heat.\\n6. Cost-effectiveness: The use of OPOs can result in lower cost compared to traditional annealing machines, which can be beneficial for solving large-scale optimization problems.\\n7. Parallelizability: The proposed machine can be easily parallelized using multiple OPO networks, which can lead to significant speedups for large-scale optimization problems.\\n8. Scalability of the problem size: The proposed machine has the potential to solve larger problem sizes compared to existing proposals, as it can scale up to larger problem sizes by simply adding more OPOs.\\n9. Accuracy: The use of non-degenerate OPOs can result in higher accuracy compared to traditional annealing machines, which can be beneficial for solving problems with high accuracy requirements.\\n10. Robustness: The proposed machine has the potential to be more robust compared to existing proposals, as it uses a photonic architecture that can operate in harsh environments.\\n\\nOverall, the proposed idea of using a non-degenerate OPO network for simulated bifurcation machines offers significant improvements over existing proposals, including scalability, speed, flexibility, noise tolerance, energy efficiency, cost-effectiveness, parallelizability, problem size scalability, accuracy, and robustness.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"question\" : q} for q in questions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
